{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f0c540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (Temp/ipykernel_1480/1084367042.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hubert\\AppData\\Local\\Temp/ipykernel_1480/1084367042.py\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    -sequential feature selection\u001b[0m\n\u001b[1;37m                                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solved:\n",
    "    -It's possible for train-test split to split data in such a way, that\n",
    "   after encoding, X_train and X_test have different numbers of features.\n",
    "   Split has to be rerun to fix it. First encode, then split again?\n",
    "   BUT IT STILL HAS TO BE ENCODED AND SPLIT BEFORE STARTING CROSS VALIDATION\n",
    "   AND SEQUENTIAL FEATURE SELECTION. MAYBE APPEND DURING SPLIT AND THEN SPLIT\n",
    "   AGAIN?\n",
    "    -Improve feature encoding to have proper ordering instead of random numbers\n",
    "    which currently influence classification accuracy:\n",
    "    https://datascience.stackexchange.com/questions/72343/encoding-with-ordinalencoder-how-to-give-levels-as-user-input\n",
    "\n",
    "Fishy:\n",
    "    -check and check for data leakage (def: https://scikit-learn.org/stable/glossary.html)\n",
    "\n",
    "TODO:\n",
    "    -add cost counting to SFS wrapper\n",
    "    -???\n",
    "    -tests and profit???\n",
    "    -report a bug with indexes when predicting X_test using audiology and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c628d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "Libs imported. Python version is:  3.9.7\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\n\\nimport numpy as np\\n\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_formatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\n\\nimport numpy as np\\n\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SKLEARN\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# works, sort of only. Possible additional commas that shouldn't be there.\n",
    "%load_ext nb_black\n",
    "\n",
    "print(\"Libs imported. Python version is: \", python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a810b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p-index\\\",\\n    \\\"labels\\\",\\n]\\n\\ncols_wine = [\\n    \\\"labels\\\",\\n    \\\"alcohol\\\",\\n    \\\"mallic-acid\\\",\\n    \\\"alcalinity\\\",\\n    \\\"ash\\\",\\n    \\\"magnesium\\\",\\n    \\\"total-phenols\\\",\\n    \\\"flavonids\\\",\\n    \\\"nonflavonid-phenols\\\",\\n    \\\"proanthocyanins\\\",\\n    \\\"color-intensity\\\",\\n    \\\"hue\\\",\\n    \\\"od-of-diluted-wines\\\",\\n    \\\"proline\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    # replace 5more in doors to 5\\n    # df_car.loc[df_car['doors'] == '5more', 'doors'] = '5'\\n    # df_car[\\\"doors\\\"] = pd.to_numeric(df_car[\\\"doors\\\"])\\n    # replace more in passengers to 5\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore-print-color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p-index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\\n\\n\\n\\\"\\\"\\\"\\nhttps://www.alldatascience.com/classification/wine-dataset-analysis-with-python/\\n1:length -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_wine():\\n    df_wine = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\\",\\n        header=None,\\n        names=cols_wine,\\n    )\\n    # index mappings\\n    # length = len(df_wine.columns)\\n    # X = df_wine.loc[:, 1:].values\\n    # y = df_wine.loc[:, 0].values\\n    return df_wine\";\n",
       "                var nbb_formatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p-index\\\",\\n    \\\"labels\\\",\\n]\\n\\ncols_wine = [\\n    \\\"labels\\\",\\n    \\\"alcohol\\\",\\n    \\\"mallic-acid\\\",\\n    \\\"alcalinity\\\",\\n    \\\"ash\\\",\\n    \\\"magnesium\\\",\\n    \\\"total-phenols\\\",\\n    \\\"flavonids\\\",\\n    \\\"nonflavonid-phenols\\\",\\n    \\\"proanthocyanins\\\",\\n    \\\"color-intensity\\\",\\n    \\\"hue\\\",\\n    \\\"od-of-diluted-wines\\\",\\n    \\\"proline\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    # replace 5more in doors to 5\\n    # df_car.loc[df_car['doors'] == '5more', 'doors'] = '5'\\n    # df_car[\\\"doors\\\"] = pd.to_numeric(df_car[\\\"doors\\\"])\\n    # replace more in passengers to 5\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore-print-color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p-index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\\n\\n\\n\\\"\\\"\\\"\\nhttps://www.alldatascience.com/classification/wine-dataset-analysis-with-python/\\n1:length -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_wine():\\n    df_wine = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\\",\\n        header=None,\\n        names=cols_wine,\\n    )\\n    # index mappings\\n    # length = len(df_wine.columns)\\n    # X = df_wine.loc[:, 1:].values\\n    # y = df_wine.loc[:, 0].values\\n    return df_wine\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utility functions\n",
    "\n",
    "cols_mushroom = [\n",
    "    \"labels\",\n",
    "    \"cap-shape\",\n",
    "    \"cap-surface\",\n",
    "    \"cap-color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill-attachment\",\n",
    "    \"gill-spacing\",\n",
    "    \"gill-size\",\n",
    "    \"gill-color\",\n",
    "    \"stalk-shape\",\n",
    "    \"stalk-root\",\n",
    "    \"stalk-surface-above-ring\",\n",
    "    \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\",\n",
    "    \"stalk-color-below-ring\",\n",
    "    \"veil-type\",\n",
    "    \"veil-color\",\n",
    "    \"ring-number\",\n",
    "    \"ring-type\",\n",
    "    \"spore-print-color\",\n",
    "    \"population\",\n",
    "    \"habitat\",\n",
    "]\n",
    "\n",
    "cols_car = [\"buying\", \"maintenance\", \"doors\", \"passengers\", \"boot\", \"safety\", \"labels\"]\n",
    "\n",
    "cols_audiology = [\n",
    "    \"age_gt_60\",\n",
    "    \"air\",\n",
    "    \"airBoneGap\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bone\",\n",
    "    \"boneAbnormal\",\n",
    "    \"bser\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"speech\",\n",
    "    \"static_normal\",\n",
    "    \"tymp\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "    \"p-index\",\n",
    "    \"labels\",\n",
    "]\n",
    "\n",
    "cols_wine = [\n",
    "    \"labels\",\n",
    "    \"alcohol\",\n",
    "    \"mallic-acid\",\n",
    "    \"alcalinity\",\n",
    "    \"ash\",\n",
    "    \"magnesium\",\n",
    "    \"total-phenols\",\n",
    "    \"flavonids\",\n",
    "    \"nonflavonid-phenols\",\n",
    "    \"proanthocyanins\",\n",
    "    \"color-intensity\",\n",
    "    \"hue\",\n",
    "    \"od-of-diluted-wines\",\n",
    "    \"proline\",\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "0-5 -> data\n",
    "6 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_car():\n",
    "    df_car = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "        header=None,\n",
    "        names=cols_car,\n",
    "    )\n",
    "    # mappings using indexes:\n",
    "    # X = df_car.loc[:, :5].values\n",
    "    # y = df_car.loc[:, 6].values\n",
    "    labels_col = df_car.pop(\"labels\")\n",
    "    df_car.insert(0, \"labels\", labels_col)\n",
    "    # replace 5more in doors to 5\n",
    "    # df_car.loc[df_car['doors'] == '5more', 'doors'] = '5'\n",
    "    # df_car[\"doors\"] = pd.to_numeric(df_car[\"doors\"])\n",
    "    # replace more in passengers to 5\n",
    "    return df_car\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "1-22 -> data\n",
    "0 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_mushroom():\n",
    "    df_mushroom = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "        header=None,\n",
    "        names=cols_mushroom,\n",
    "    )\n",
    "    # index mappings\n",
    "    # X = df_mushroom.loc[:, 1:].values\n",
    "    # y = df_mushroom.loc[:, 0].values\n",
    "    df_mushroom = df_mushroom.drop(\"odor\", axis=1)\n",
    "    df_mushroom = df_mushroom.drop(\"spore-print-color\", axis=1)\n",
    "    return df_mushroom\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\n",
    "0:length-2 -> data\n",
    "length-1 unique id (p1-p200)\n",
    "length -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_audiology():\n",
    "    df_audiology = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\",\n",
    "        header=None,\n",
    "        names=cols_audiology,\n",
    "    )\n",
    "    # index mapping\n",
    "    # length = len(df_audiology.columns)\n",
    "    # X = df_audiology.loc[:, : length - 3].values\n",
    "    # y = df_audiology.loc[:, length - 1].values\n",
    "    df_audiology = df_audiology.drop(\"p-index\", axis=1)\n",
    "    labels_col = df_audiology.pop(\"labels\")\n",
    "    df_audiology.insert(0, \"labels\", labels_col)\n",
    "    return df_audiology\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://www.alldatascience.com/classification/wine-dataset-analysis-with-python/\n",
    "1:length -> data\n",
    "0 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_wine():\n",
    "    df_wine = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\",\n",
    "        header=None,\n",
    "        names=cols_wine,\n",
    "    )\n",
    "    # index mappings\n",
    "    # length = len(df_wine.columns)\n",
    "    # X = df_wine.loc[:, 1:].values\n",
    "    # y = df_wine.loc[:, 0].values\n",
    "    return df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3ea44b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   labels                    8124 non-null   object\n",
      " 1   cap-shape                 8124 non-null   object\n",
      " 2   cap-surface               8124 non-null   object\n",
      " 3   cap-color                 8124 non-null   object\n",
      " 4   bruises                   8124 non-null   object\n",
      " 5   gill-attachment           8124 non-null   object\n",
      " 6   gill-spacing              8124 non-null   object\n",
      " 7   gill-size                 8124 non-null   object\n",
      " 8   gill-color                8124 non-null   object\n",
      " 9   stalk-shape               8124 non-null   object\n",
      " 10  stalk-root                8124 non-null   object\n",
      " 11  stalk-surface-above-ring  8124 non-null   object\n",
      " 12  stalk-surface-below-ring  8124 non-null   object\n",
      " 13  stalk-color-above-ring    8124 non-null   object\n",
      " 14  stalk-color-below-ring    8124 non-null   object\n",
      " 15  veil-type                 8124 non-null   object\n",
      " 16  veil-color                8124 non-null   object\n",
      " 17  ring-number               8124 non-null   object\n",
      " 18  ring-type                 8124 non-null   object\n",
      " 19  population                8124 non-null   object\n",
      " 20  habitat                   8124 non-null   object\n",
      "dtypes: object(21)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "Size of X:  (8124, 20)\n",
      "Size of y:  (8124,)\n",
      "Size of dataset costs:  (1, 20)\n",
      "Cost of classification on full dataset:  97736\n",
      "Data has been split.\n",
      "Labels encoded:  (6499,) ,  (1625,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n### dataset = load_wine() # all cols numerical, doesn't work\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_formatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n### dataset = load_wine() # all cols numerical, doesn't work\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "# dataset = load_car()\n",
    "dataset = load_mushroom()\n",
    "# dataset = load_audiology()\n",
    "### dataset = load_wine() # all cols numerical, doesn't work\n",
    "\n",
    "print(dataset.info())\n",
    "# print(\"First five records:\")\n",
    "# print(dataset.head())\n",
    "\n",
    "# Extract to X and y\n",
    "X_cat = dataset.loc[:, dataset.columns != \"labels\"]\n",
    "y_cat = dataset.loc[:, \"labels\"]\n",
    "\n",
    "print(\"Size of X: \", np.shape(X_cat))\n",
    "print(\"Size of y: \", np.shape(y_cat))\n",
    "\n",
    "# Generate a matrix of costs\n",
    "max_cost_allowed = 10000\n",
    "\n",
    "dataset_costs = pd.DataFrame(\n",
    "    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\n",
    "    columns=X_cat.columns,\n",
    ")\n",
    "\n",
    "print(\"Size of dataset costs: \", np.shape(dataset_costs))\n",
    "print(\"Cost of classification on full dataset: \", dataset_costs.sum(axis=1)[0])\n",
    "\n",
    "max_seed_val = 2 ** 32 - 1\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\n",
    ")\n",
    "print(\"Data has been split.\")\n",
    "# print(\"X contains features: \", X_train.columns == \"index\")\n",
    "\n",
    "# Transform y using label encoder\n",
    "le = LabelEncoder().fit(y_cat)\n",
    "encoded_y_train = le.transform(y_train)\n",
    "encoded_y_test = le.transform(y_test)\n",
    "print(\"Labels encoded: \", np.shape(encoded_y_train), \", \", np.shape(encoded_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba83093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols created.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_formatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collectors of values\n",
    "\n",
    "cols_one_hot = [\n",
    "    \"cap-shape\",\n",
    "    \"cap-surface\",\n",
    "    \"cap-color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill-attachment\",\n",
    "    \"gill-spacing\",\n",
    "    \"gill-size\",\n",
    "    \"gill-color\",\n",
    "    \"stalk-shape\",\n",
    "    \"stalk-root\",\n",
    "    \"stalk-surface-above-ring\",\n",
    "    \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\",\n",
    "    \"stalk-color-below-ring\",\n",
    "    \"veil-type\",\n",
    "    \"veil-color\",\n",
    "    \"ring-number\",\n",
    "    \"ring-type\",\n",
    "    \"spore-print-color\",\n",
    "    \"habitat\",\n",
    "    \"age_gt_60\",\n",
    "    \"airBoneGap\",\n",
    "    \"boneAbnormal\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"static_normal\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "]\n",
    "\n",
    "cols_ordinal = [\n",
    "    \"buying\",\n",
    "    \"maintenance\",\n",
    "    \"doors\",\n",
    "    \"passengers\",\n",
    "    \"boot\",\n",
    "    \"safety\",\n",
    "    \"population\",\n",
    "    \"air\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bser\",\n",
    "    \"bone\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"speech\",\n",
    "    \"tymp\",\n",
    "]\n",
    "\n",
    "print(\"Cols created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6bfe05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order created.\n",
      "    buying maintenance    doors passengers     boot   safety population  \\\n",
      "0      low         low        2          2    small      low          y   \n",
      "1      med         med        3          4      med      med          v   \n",
      "2     high        high        4       more      big     high          s   \n",
      "3    vhigh       vhigh    5more    filler1  filler1  filler1          n   \n",
      "4  filler1     filler1  filler1    filler2  filler2  filler2          c   \n",
      "5  filler2     filler2  filler2    filler3  filler3  filler3          a   \n",
      "6  filler3     filler3  filler3    filler4  filler4  filler4    filler1   \n",
      "\n",
      "        air      ar_c      ar_u      bser        bone    o_ar_c    o_ar_u  \\\n",
      "0    normal         ?         ?         ?           ?         ?         ?   \n",
      "1      mild    absent    absent    normal  unmeasured    absent    absent   \n",
      "2  moderate    normal    normal  degraded      normal    normal    normal   \n",
      "3    severe  elevated  elevated   filler1        mild  elevated  elevated   \n",
      "4  profound   filler1   filler1   filler2    moderate   filler1   filler1   \n",
      "5   filler1   filler2   filler2   filler3     filler1   filler2   filler2   \n",
      "6   filler2   filler3   filler3   filler4     filler3   filler3   filler3   \n",
      "\n",
      "       speech     tymp  \n",
      "0           ?        a  \n",
      "1  unmeasured       as  \n",
      "2   very_poor        b  \n",
      "3        poor       ad  \n",
      "4      normal        c  \n",
      "5        good  filler1  \n",
      "6   very_good  filler2  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_formatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make order of categories per each column in ordinal_columns\n",
    "order_of_ordinal_categories = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"buying\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"maintenance\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"doors\": [\"2\", \"3\", \"4\", \"5more\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"passengers\": [\"2\", \"4\", \"more\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"boot\": [\"small\", \"med\", \"big\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"safety\": [\"low\", \"med\", \"high\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"population\": [\"y\", \"v\", \"s\", \"n\", \"c\", \"a\", \"filler1\"],\n",
    "        \"air\": [\n",
    "            \"normal\",\n",
    "            \"mild\",\n",
    "            \"moderate\",\n",
    "            \"severe\",\n",
    "            \"profound\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "        ],\n",
    "        \"ar_c\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"ar_u\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"bser\": [\"?\", \"normal\", \"degraded\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"bone\": [\"?\", \"unmeasured\", \"normal\", \"mild\", \"moderate\", \"filler1\", \"filler3\"],\n",
    "        \"o_ar_c\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"o_ar_u\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"speech\": [\n",
    "            \"?\",\n",
    "            \"unmeasured\",\n",
    "            \"very_poor\",\n",
    "            \"poor\",\n",
    "            \"normal\",\n",
    "            \"good\",\n",
    "            \"very_good\",\n",
    "        ],\n",
    "        \"tymp\": [\"a\", \"as\", \"b\", \"ad\", \"c\", \"filler1\", \"filler2\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Order created.\")\n",
    "print(order_of_ordinal_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3c4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class EncodingCategoricalBayes has been created\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_formatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create custom encoding categorical bayes classifier\n",
    "class EncodingCategoricalBayes:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # classifier,\n",
    "        ordinal_categories_order,\n",
    "        ordinal_columns,\n",
    "        one_hot_columns,\n",
    "        dataset,\n",
    "    ):\n",
    "        # self.classifier = classifier\n",
    "        self.ordinal_categories_order = ordinal_categories_order\n",
    "        self.ordinal_columns = ordinal_columns\n",
    "        self.one_hot_columns = one_hot_columns\n",
    "        self.transformer_dataset = dataset\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier = CategoricalNB(min_categories=X.shape[0])\n",
    "        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\n",
    "        self.column_transformer.fit(X)\n",
    "        return self.classifier.fit(self.encode_features(X), y)\n",
    "        # return self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(self.encode_features(X))\n",
    "        # return self.classifier.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.classifier.predict_proba(self.encode_features(X))\n",
    "        # return self.classifier.predict_proba(X)\n",
    "\n",
    "    def encode_features(self, X):\n",
    "        encoded_X = self.column_transformer.transform(X)\n",
    "        if scipy.sparse.issparse(encoded_X):\n",
    "            encoded_X = encoded_X.toarray()\n",
    "        return encoded_X\n",
    "\n",
    "    def make_column_transformer(self, X):\n",
    "        # Get current ordinal and one hot columns\n",
    "        total_column_list = X.select_dtypes(include=\"object\").columns\n",
    "        # print(\"Total col list: \", total_column_list)\n",
    "        current_columns_one_hot = self.collect_current_one_hot_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "        current_columns_ordinal = self.collect_current_ordinal_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "\n",
    "        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\n",
    "            current_columns_ordinal\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"Columns in column transformer: \",\n",
    "            current_columns_one_hot,\n",
    "            \" and \",\n",
    "            current_columns_ordinal,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Create column transformer\n",
    "        column_transformer = make_column_transformer(\n",
    "            (OneHotEncoder(), current_columns_one_hot),\n",
    "            (\n",
    "                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\n",
    "                current_columns_ordinal,\n",
    "            ),\n",
    "        )\n",
    "        return column_transformer\n",
    "\n",
    "    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\n",
    "        # Get common cols to feed them in proper order to ordinal encoder\n",
    "        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\n",
    "            argColumns\n",
    "        )\n",
    "        # Convert to list\n",
    "        order_of_ordinal_categories_list = (\n",
    "            self.ordinal_categories_order[index_of_common_cols]\n",
    "            .values.transpose()\n",
    "            .tolist()\n",
    "        )\n",
    "        return order_of_ordinal_categories_list\n",
    "\n",
    "    def intersection(self, lst1, lst2):\n",
    "        # collects common elements in both lists\n",
    "        return [value for value in lst1 if value in lst2]\n",
    "\n",
    "    def collect_current_one_hot_columns(self, argCols):\n",
    "        return self.intersection(self.one_hot_columns, argCols)\n",
    "\n",
    "    def collect_current_ordinal_columns(self, argCols):\n",
    "        # make list of all values and create steps for them\n",
    "        return self.intersection(self.ordinal_columns, argCols)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.classifier.get_params()\n",
    "\n",
    "\n",
    "print(\"Class EncodingCategoricalBayes has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd1b940f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Forward Feature Selector is created.\n",
      "Outcomes: <class 'pandas.core.frame.DataFrame'>\n",
      "     classified_class\n",
      "6830                p\n",
      "4872                e\n",
      "2700                e\n",
      "7876                p\n",
      "8089                p\n",
      "...               ...\n",
      "359                 e\n",
      "642                 e\n",
      "3378                e\n",
      "4912                p\n",
      "411                 e\n",
      "\n",
      "[1625 rows x 1 columns]\n",
      "Highest prbs: <class 'pandas.core.frame.DataFrame'>\n",
      "      highest_probas\n",
      "6830        0.999251\n",
      "4872        0.737010\n",
      "2700        0.948305\n",
      "7876        0.999251\n",
      "8089        0.999251\n",
      "...              ...\n",
      "359         0.894273\n",
      "642         0.894273\n",
      "3378        0.894273\n",
      "4912        0.620600\n",
      "411         0.948305\n",
      "\n",
      "[1625 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/2176907102.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m selector.sequential_predict(\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mencoded_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/2176907102.py\u001b[0m in \u001b[0;36msequential_predict\u001b[1;34m(self, X_train_original, y_train_original, X_test_original, y_test_original, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhighest_probas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnull\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m# 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'null' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 86;\n",
       "                var nbb_unformatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n    ):\\n        # Make copies\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n\\n        duplicates = pd.DataFrame()\\n\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # main loop, until all test classes are classified\\n        while unused_features:\\n            # make a list of features with their predicted accuracy\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas = self.predict_proba_wrapper(\\n                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates\\n            )\\n\\n            print(\\\"Outcomes:\\\", type(outcomes))\\n            print(outcomes)\\n            print(\\\"Highest prbs:\\\", type(highest_probas))\\n            print(highest_probas)\\n\\n            return null\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_test)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            feature_series = (\\n                current_features * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"features_used\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_proba[\\\"highest_probas\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n            rows = highest_proba.loc[condition].index\\n            inverse_rows = X_tst.index.difference(rows)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(inverse_rows)[0])\\n            X_tst.drop(rows, inplace=True)\\n            y_tst.drop(rows, inplace=True)\\n            a = pd.DataFrame(columns=final_result_columns)\\n            final_result_dataframe.append(a, inplace=True)\\n\\n            # print(\\\"final: \\\")\\n            # print(final_result_dataframe)\\n\\n            # now go back to the beginning of the loop and check for unclassified classes\\n            return stopper\\n\\n    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"classified_class\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_probas\\\"])\\n        for index, row_entry in X_test.iterrows():\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                # X_train, y_train, duplicates[index]\\n                X_train,\\n                y_train,\\n                duplicates,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"classified_class\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_probas\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn\\n\\n    def prepare_train_dataset(self, X_train, y_train, duplicates_per_case):\\n        if duplicates_per_case.empty:\\n            return X_train, y_train\\n\\n        \\\"\\\"\\\"\\n        we take first feature and duplicate the rows with that feature\\n        then another (together with the previously duplicated features)\\n        \\\"\\\"\\\"\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = X_train.insert(y_train)\\n        # for each feature\\n        for col_name in duplicates_per_case.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            print(\\\"Query:\\\", query)\\n            print(full_test_data.query(query))\\n            return null\\n            full_test_data.append(full_test_data.query(query), ignore_index=True)\\n\\n        \\\"\\\"\\\"\\n        # we can also easily exclude the appended features like below\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\\n        print(duplicate_rows)\\n        full_test_data.append(duplicate_rows, ignore_index=True)\\n        \\\"\\\"\\\"\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    \\\"\\\"\\\" Think this is not needed\\n    def classify_per_feature_and_duplicate_entries(\\n        self, X_train, y_train, X_test, y_test, next_feature, previous_features=None,\\n    ):\\n        # make feature subset\\n        features = previous_features + next_feature\\n        # create X_train subset with apropriate features\\n        X_train_subset_full = pd.DataFrame(X_train[features])\\n        X_test_subset_full = pd.DataFrame(y_train[features])\\n        # make new classifier (due to different data split when fitting), fit and predict_probas\\n        classifier = self.make_encoding_categorical_bayes(\\n            ordinal_categoires_order,\\n            cols_ordinal,\\n            cols_one_hot,\\n            pd.DataFrame(dataset[features]),\\n        )\\n        classifier.fit(X_train_subset_full, y_train)\\n        outcome = pd.DataFrame(\\n            classifier.predict(X_test_subset_full),\\n            columns=[\\\"classified_class\\\"],\\n            index=X_test.index,\\n        )\\n        probas = classifier.predict_proba(X_test_subset_full)\\n        highest_proba = pd.DataFrame(\\n            np.max(probas, axis=1), columns=[\\\"highest_probas\\\"], index=X_tst.index\\n        )\\n        return\\n    \\\"\\\"\\\"\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\\n\\n# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nselector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\";\n",
       "                var nbb_formatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n    ):\\n        # Make copies\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n\\n        duplicates = pd.DataFrame()\\n\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # main loop, until all test classes are classified\\n        while unused_features:\\n            # make a list of features with their predicted accuracy\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas = self.predict_proba_wrapper(\\n                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates\\n            )\\n\\n            print(\\\"Outcomes:\\\", type(outcomes))\\n            print(outcomes)\\n            print(\\\"Highest prbs:\\\", type(highest_probas))\\n            print(highest_probas)\\n\\n            return null\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_test)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            feature_series = (\\n                current_features * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"features_used\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_proba[\\\"highest_probas\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n            rows = highest_proba.loc[condition].index\\n            inverse_rows = X_tst.index.difference(rows)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(inverse_rows)[0])\\n            X_tst.drop(rows, inplace=True)\\n            y_tst.drop(rows, inplace=True)\\n            a = pd.DataFrame(columns=final_result_columns)\\n            final_result_dataframe.append(a, inplace=True)\\n\\n            # print(\\\"final: \\\")\\n            # print(final_result_dataframe)\\n\\n            # now go back to the beginning of the loop and check for unclassified classes\\n            return stopper\\n\\n    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"classified_class\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_probas\\\"])\\n        for index, row_entry in X_test.iterrows():\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                # X_train, y_train, duplicates[index]\\n                X_train,\\n                y_train,\\n                duplicates,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"classified_class\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_probas\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn\\n\\n    def prepare_train_dataset(self, X_train, y_train, duplicates_per_case):\\n        if duplicates_per_case.empty:\\n            return X_train, y_train\\n\\n        \\\"\\\"\\\"\\n        we take first feature and duplicate the rows with that feature\\n        then another (together with the previously duplicated features)\\n        \\\"\\\"\\\"\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = X_train.insert(y_train)\\n        # for each feature\\n        for col_name in duplicates_per_case.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            print(\\\"Query:\\\", query)\\n            print(full_test_data.query(query))\\n            return null\\n            full_test_data.append(full_test_data.query(query), ignore_index=True)\\n\\n        \\\"\\\"\\\"\\n        # we can also easily exclude the appended features like below\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\\n        print(duplicate_rows)\\n        full_test_data.append(duplicate_rows, ignore_index=True)\\n        \\\"\\\"\\\"\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    \\\"\\\"\\\" Think this is not needed\\n    def classify_per_feature_and_duplicate_entries(\\n        self, X_train, y_train, X_test, y_test, next_feature, previous_features=None,\\n    ):\\n        # make feature subset\\n        features = previous_features + next_feature\\n        # create X_train subset with apropriate features\\n        X_train_subset_full = pd.DataFrame(X_train[features])\\n        X_test_subset_full = pd.DataFrame(y_train[features])\\n        # make new classifier (due to different data split when fitting), fit and predict_probas\\n        classifier = self.make_encoding_categorical_bayes(\\n            ordinal_categoires_order,\\n            cols_ordinal,\\n            cols_one_hot,\\n            pd.DataFrame(dataset[features]),\\n        )\\n        classifier.fit(X_train_subset_full, y_train)\\n        outcome = pd.DataFrame(\\n            classifier.predict(X_test_subset_full),\\n            columns=[\\\"classified_class\\\"],\\n            index=X_test.index,\\n        )\\n        probas = classifier.predict_proba(X_test_subset_full)\\n        highest_proba = pd.DataFrame(\\n            np.max(probas, axis=1), columns=[\\\"highest_probas\\\"], index=X_tst.index\\n        )\\n        return\\n    \\\"\\\"\\\"\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\\n\\n# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nselector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sequential Forward Feature Selector\n",
    "class SequentialForwardFeatureSelector:\n",
    "    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\n",
    "        self.classification_costs = classification_costs\n",
    "        self.CV_folds = CV_folds\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "\n",
    "    def sequential_predict(\n",
    "        self,\n",
    "        X_train_original,\n",
    "        y_train_original,\n",
    "        X_test_original,\n",
    "        y_test_original,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "        whole_dataset,\n",
    "    ):\n",
    "        # Make copies\n",
    "        X_tr = copy.deepcopy(X_train_original)\n",
    "        y_tr = copy.deepcopy(y_train_original)\n",
    "        X_tst = copy.deepcopy(X_test_original)\n",
    "        y_tst = copy.deepcopy(y_test_original)\n",
    "\n",
    "        final_result_columns = [\n",
    "            \"highest_proba\",\n",
    "            \"outcome\",\n",
    "            \"cost_of_classification\",\n",
    "            \"feature_used_to_classify\",\n",
    "        ]\n",
    "\n",
    "        duplicates = pd.DataFrame()\n",
    "\n",
    "        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\n",
    "\n",
    "        unused_features = X_tst.columns.tolist()\n",
    "        current_features = []\n",
    "\n",
    "        # main loop, until all test classes are classified\n",
    "        while unused_features:\n",
    "            # make a list of features with their predicted accuracy\n",
    "            accuracy_per_new_feature = self.find_next_best_feature(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                unused_features,\n",
    "                current_features,\n",
    "                whole_dataset,\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "            )\n",
    "\n",
    "            # we have all features and their accuracies, we pick the best one\n",
    "            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\n",
    "            # and adjust feature trackers\n",
    "            unused_features.remove(best_next_feature)\n",
    "            current_features.append(best_next_feature)\n",
    "\n",
    "            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\n",
    "\n",
    "            # create X_train subset with apropriate features\n",
    "            X_train_subset_full = pd.DataFrame(X_tr[current_features])\n",
    "            X_test_subset_full = pd.DataFrame(X_tst[current_features])\n",
    "            # make new classifier (due to different encoder data)\n",
    "            classifier = self.make_encoding_categorical_bayes(\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "                pd.DataFrame(dataset[current_features]),\n",
    "            )\n",
    "            # define duplicates before end of refactorization and moving forward\n",
    "            outcomes, highest_probas = self.predict_proba_wrapper(\n",
    "                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates\n",
    "            )\n",
    "\n",
    "            return null\n",
    "        \n",
    "            ##### PICKUP: go further down the algorithm making sure it works ok\n",
    "\n",
    "            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\n",
    "\n",
    "            cost_series = (\n",
    "                [self.get_classification_costs(current_features)] * np.shape(X_test)[0]\n",
    "            )  # remember to change to list of featureS\n",
    "            cost_of_classification = pd.DataFrame(\n",
    "                cost_series, columns=[\"cost\"], index=X_tst.index\n",
    "            )\n",
    "            # print(cost_of_classification)\n",
    "\n",
    "            feature_series = (\n",
    "                current_features * np.shape(X_tst)[0]\n",
    "            )  # remember to change to list of featureS\n",
    "            feature_used_to_classify = pd.DataFrame(\n",
    "                feature_series, columns=[\"features_used\"], index=X_tst.index\n",
    "            )\n",
    "            # print(feature_used_to_classify)\n",
    "\n",
    "            # remove already classified classes\n",
    "            condition = highest_proba[\"highest_probas\"] > (\n",
    "                1 - self.uncertainty_threshold\n",
    "            )\n",
    "            rows = highest_proba.loc[condition].index\n",
    "            inverse_rows = X_tst.index.difference(rows)\n",
    "            y_tst = pd.DataFrame(y_tst, columns=[\"labels\"], index=X_tst.index)\n",
    "            # print(\"Number of rows to drop:\", np.shape(rows)[0])\n",
    "            # print(\"Number of rows to leave:\", np.shape(inverse_rows)[0])\n",
    "            X_tst.drop(rows, inplace=True)\n",
    "            y_tst.drop(rows, inplace=True)\n",
    "            a = pd.DataFrame(columns=final_result_columns)\n",
    "            final_result_dataframe.append(a, inplace=True)\n",
    "\n",
    "            # print(\"final: \")\n",
    "            # print(final_result_dataframe)\n",
    "\n",
    "            # now go back to the beginning of the loop and check for unclassified classes\n",
    "            return stopper\n",
    "\n",
    "    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates):\n",
    "        outcomes_fn = pd.DataFrame(columns=[\"classified_class\"])\n",
    "        highest_probas_fn = pd.DataFrame(columns=[\"highest_probas\"])\n",
    "        for index, row_entry in X_test.iterrows():\n",
    "            X_train_mod, y_train_mod = self.prepare_train_dataset(\n",
    "                # X_train, y_train, duplicates[index]\n",
    "                X_train,\n",
    "                y_train,\n",
    "                duplicates,\n",
    "            )\n",
    "            classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "            df_row_entry = row_entry.to_frame().T\n",
    "\n",
    "            # gotta make it in 2 steps bc of no column name tracking in numpy\n",
    "            new_outcome_df = pd.DataFrame(\n",
    "                classifier.predict(df_row_entry),\n",
    "                columns=[\"classified_class\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "\n",
    "            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\n",
    "\n",
    "            probas_fn = classifier.predict_proba(df_row_entry)\n",
    "            new_probas_df = pd.DataFrame(\n",
    "                np.max(np.max(probas_fn, axis=1), axis=0),\n",
    "                columns=[\"highest_probas\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\n",
    "        return outcomes_fn, highest_probas_fn\n",
    "\n",
    "    def prepare_train_dataset(self, X_train, y_train, duplicates_per_case):\n",
    "        if duplicates_per_case.empty:\n",
    "            return X_train, y_train\n",
    "\n",
    "        \"\"\"\n",
    "        we take first feature and duplicate the rows with that feature\n",
    "        then another (together with the previously duplicated features)\n",
    "        \"\"\"\n",
    "\n",
    "        # make 1 full dataset for easy modification\n",
    "        full_test_data = X_train.insert(y_train)\n",
    "        # for each feature\n",
    "        for col_name in duplicates_per_case.columns.tolist():\n",
    "            # create query\n",
    "            query = col_name + \"=\" + str(full_test_data[col_name][0])\n",
    "            print(\"Query:\", query)\n",
    "            print(full_test_data.query(query))\n",
    "            return null\n",
    "            full_test_data.append(full_test_data.query(query), ignore_index=True)\n",
    "\n",
    "        \"\"\"\n",
    "        # we can also easily exclude the appended features like below\n",
    "        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\n",
    "        for col_name in duplicates.columns.tolist():\n",
    "            # create query\n",
    "            query = col_name + \"=\" + str(full_test_data[col_name][0])\n",
    "            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\n",
    "        print(duplicate_rows)\n",
    "        full_test_data.append(duplicate_rows, ignore_index=True)\n",
    "        \"\"\"\n",
    "        # return X_train, y_train\n",
    "        return (\n",
    "            full_test_data.loc[:, full_test_data.columns != \"labels\"],\n",
    "            full_test_data.loc[:, \"labels\"],\n",
    "        )\n",
    "\n",
    "    \"\"\" Think this is not needed\n",
    "    def classify_per_feature_and_duplicate_entries(\n",
    "        self, X_train, y_train, X_test, y_test, next_feature, previous_features=None,\n",
    "    ):\n",
    "        # make feature subset\n",
    "        features = previous_features + next_feature\n",
    "        # create X_train subset with apropriate features\n",
    "        X_train_subset_full = pd.DataFrame(X_train[features])\n",
    "        X_test_subset_full = pd.DataFrame(y_train[features])\n",
    "        # make new classifier (due to different data split when fitting), fit and predict_probas\n",
    "        classifier = self.make_encoding_categorical_bayes(\n",
    "            ordinal_categoires_order,\n",
    "            cols_ordinal,\n",
    "            cols_one_hot,\n",
    "            pd.DataFrame(dataset[features]),\n",
    "        )\n",
    "        classifier.fit(X_train_subset_full, y_train)\n",
    "        outcome = pd.DataFrame(\n",
    "            classifier.predict(X_test_subset_full),\n",
    "            columns=[\"classified_class\"],\n",
    "            index=X_test.index,\n",
    "        )\n",
    "        probas = classifier.predict_proba(X_test_subset_full)\n",
    "        highest_proba = pd.DataFrame(\n",
    "            np.max(probas, axis=1), columns=[\"highest_probas\"], index=X_tst.index\n",
    "        )\n",
    "        return\n",
    "    \"\"\"\n",
    "\n",
    "    def find_next_best_feature(\n",
    "        self,\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        unused_feat,\n",
    "        current_feat,\n",
    "        whole_dataset,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "    ):\n",
    "        kf = KFold(n_splits=self.CV_folds)\n",
    "        accuracy_per_new_feature = pd.DataFrame(\n",
    "            0, index=np.arange(1), columns=unused_feat,\n",
    "        )\n",
    "        for new_feature in unused_feat:\n",
    "            # print(\"Calculating feature: \", new_feature)\n",
    "            sum_of_accuracies = 0\n",
    "            feature_set_to_try = copy.deepcopy(current_feat)\n",
    "            feature_set_to_try.append(new_feature)\n",
    "            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\n",
    "\n",
    "            for train_index, test_index in kf.split(X_tr):\n",
    "                # create _train, _cv_test, _test splits\n",
    "                # no need to reshuffle it, it's already in random order\n",
    "                # X is a dataframe\n",
    "                X_train, X_cv = (\n",
    "                    X_tr.iloc[train_index],\n",
    "                    X_tr.iloc[test_index],\n",
    "                )\n",
    "\n",
    "                # y is a numpy array\n",
    "                y_train, y_cv = (\n",
    "                    y_tr[train_index],\n",
    "                    y_tr[test_index],\n",
    "                )\n",
    "\n",
    "                # print(\"train: \", train_index, \"test: \", test_index)\n",
    "\n",
    "                # add feture to test to X\n",
    "                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\n",
    "                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\n",
    "\n",
    "                # make classifier\n",
    "                classifier = self.make_encoding_categorical_bayes(\n",
    "                    ordinal_categoires_order,\n",
    "                    cols_ordinal,\n",
    "                    cols_one_hot,\n",
    "                    dataset_for_encoder,\n",
    "                )\n",
    "\n",
    "                # train classifier\n",
    "                classifier.fit(X_train_subset, y_train)\n",
    "                # predict\n",
    "                y_cv_prediciton = classifier.predict(X_cv_subset)\n",
    "                # judge accuracy of new feature subset\n",
    "                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\n",
    "\n",
    "            # save accuracy per new feature\n",
    "            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\n",
    "            # print(\"Finished calculations for feature: \",new_feature,\"Accuracy: \",accuracy_per_new_feature[new_feature][0])\n",
    "\n",
    "        return accuracy_per_new_feature\n",
    "\n",
    "    def make_encoding_categorical_bayes(\n",
    "        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\n",
    "    ):\n",
    "        return EncodingCategoricalBayes(\n",
    "            # classifier=CategoricalNB(),\n",
    "            ordinal_categories_order=order_of_ordinal_categories,\n",
    "            ordinal_columns=cols_ordinal,\n",
    "            one_hot_columns=cols_one_hot,\n",
    "            dataset=whole_dataset,\n",
    "        )\n",
    "\n",
    "    def get_classification_costs(self, list_of_categories):\n",
    "        return self.classification_costs[\n",
    "            self.classification_costs.columns.intersection(list_of_categories)\n",
    "        ].sum(axis=1)[0]\n",
    "\n",
    "\n",
    "print(\"Sequential Forward Feature Selector is created.\")\n",
    "\n",
    "# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\n",
    "\n",
    "selector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\n",
    "\n",
    "# Train the model using the training sets\n",
    "selector.sequential_predict(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    X_test,\n",
    "    encoded_y_test,\n",
    "    order_of_ordinal_categories,\n",
    "    cols_ordinal,\n",
    "    cols_one_hot,\n",
    "    X_cat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36541769",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16992/1153426140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m selector.sequential_predict(\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mencoded_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16992/1351503649.py\u001b[0m in \u001b[0;36msequential_predict\u001b[1;34m(self, X_tr, y_tr, X_test, y_test, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset, previous_features)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# create _train, _cv_test, _test splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_cv_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cv_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10)\\n\\n# Train the model using the training sets\\n\\nprint(type(X_train))\\nprint(type(encoded_y_train))\\nprint(type(X_test))\\nprint(type(encoded_y_test))\\n\\nselector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\\n\\n\\\"\\\"\\\"\\nnbayes = CategoricalNB(min_categories=X_train.shape[0])\\n\\nenb = EncodingCategoricalBayes(\\n    classifier=nbayes,\\n    ordinal_categories_order=order_of_ordinal_categories,\\n    ordinal_columns=cols_ordinal,\\n    one_hot_columns=cols_one_hot,\\n    dataset=X_cat,\\n)\\n\\n# Train the model using the training sets\\nenb.fit(X_train, encoded_y_train)\\n\\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10)\\n\\n# Train the model using the training sets\\n\\nprint(type(X_train))\\nprint(type(encoded_y_train))\\nprint(type(X_test))\\nprint(type(encoded_y_test))\\n\\nselector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\\n\\n\\\"\\\"\\\"\\nnbayes = CategoricalNB(min_categories=X_train.shape[0])\\n\\nenb = EncodingCategoricalBayes(\\n    classifier=nbayes,\\n    ordinal_categories_order=order_of_ordinal_categories,\\n    ordinal_columns=cols_ordinal,\\n    one_hot_columns=cols_one_hot,\\n    dataset=X_cat,\\n)\\n\\n# Train the model using the training sets\\nenb.fit(X_train, encoded_y_train)\\n\\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\n",
    "\n",
    "selector = SequentialForwardFeatureSelector(dataset_costs, 10)\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(encoded_y_train))\n",
    "print(type(X_test))\n",
    "print(type(encoded_y_test))\n",
    "\n",
    "selector.sequential_predict(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    X_test,\n",
    "    encoded_y_test,\n",
    "    order_of_ordinal_categories,\n",
    "    cols_ordinal,\n",
    "    cols_one_hot,\n",
    "    X_cat,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "nbayes = CategoricalNB(min_categories=X_train.shape[0])\n",
    "\n",
    "enb = EncodingCategoricalBayes(\n",
    "    classifier=nbayes,\n",
    "    ordinal_categories_order=order_of_ordinal_categories,\n",
    "    ordinal_columns=cols_ordinal,\n",
    "    one_hot_columns=cols_one_hot,\n",
    "    dataset=X_cat,\n",
    ")\n",
    "\n",
    "# Train the model using the training sets\n",
    "enb.fit(X_train, encoded_y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6583060e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99924009 0.00075991]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"\\n\\n# Predict the response for test dataset\\ny_pred = le.inverse_transform(enb.predict(X_test))\\ny_pred_probas = enb.predict_proba(X_test)\\nprint(y_pred_probas[0:5])\";\n",
       "                var nbb_formatted_code = \"# Predict the response for test dataset\\ny_pred = le.inverse_transform(enb.predict(X_test))\\ny_pred_probas = enb.predict_proba(X_test)\\nprint(y_pred_probas[0:5])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the response for test dataset\n",
    "y_pred = le.inverse_transform(enb.predict(X_test))\n",
    "y_pred_probas = enb.predict_proba(X_test)\n",
    "print(y_pred_probas[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d835b50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.33846153846153 %\n",
      "F1 score: 90.22421058416137 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# Model Accuracy, how often is the classifier correct?\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_formatted_code = \"# Model Accuracy, how often is the classifier correct?\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred) * 100, \"%\")\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred, average=\"weighted\") * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac59494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130986\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Get cost of classification\\nclassification_cost = dataset_costs[X_cat.columns].sum(axis=1)[0]\\nprint(classification_cost)\";\n",
       "                var nbb_formatted_code = \"# Get cost of classification\\nclassification_cost = dataset_costs[X_cat.columns].sum(axis=1)[0]\\nprint(classification_cost)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get cost of classification\n",
    "classification_cost = dataset_costs[X_cat.columns].sum(axis=1)[0]\n",
    "print(classification_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c58be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
