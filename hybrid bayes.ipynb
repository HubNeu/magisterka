{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f0c540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (Temp/ipykernel_1480/1084367042.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hubert\\AppData\\Local\\Temp/ipykernel_1480/1084367042.py\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    -sequential feature selection\u001b[0m\n\u001b[1;37m                                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solved:\n",
    "    -It's possible for train-test split to split data in such a way, that\n",
    "   after encoding, X_train and X_test have different numbers of features.\n",
    "   Split has to be rerun to fix it. First encode, then split again?\n",
    "   BUT IT STILL HAS TO BE ENCODED AND SPLIT BEFORE STARTING CROSS VALIDATION\n",
    "   AND SEQUENTIAL FEATURE SELECTION. MAYBE APPEND DURING SPLIT AND THEN SPLIT\n",
    "   AGAIN?\n",
    "    -Improve feature encoding to have proper ordering instead of random numbers\n",
    "    which currently influence classification accuracy:\n",
    "    https://datascience.stackexchange.com/questions/72343/encoding-with-ordinalencoder-how-to-give-levels-as-user-input\n",
    "\n",
    "Fishy:\n",
    "    -check and check for data leakage (def: https://scikit-learn.org/stable/glossary.html)\n",
    "\n",
    "TODO:\n",
    "    -add cost counting to SFS wrapper\n",
    "    -???\n",
    "    -tests and profit???\n",
    "    -report a bug with indexes when predicting X_test using audiology and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c628d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs imported. Python version is:  3.9.7\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\n\\nimport numpy as np\\n\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_formatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\n\\nimport numpy as np\\n\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SKLEARN\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# works, sort of only. Possible additional commas that shouldn't be there.\n",
    "%load_ext nb_black\n",
    "\n",
    "print(\"Libs imported. Python version is: \", python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a810b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p-index\\\",\\n    \\\"labels\\\",\\n]\\n\\ncols_wine = [\\n    \\\"labels\\\",\\n    \\\"alcohol\\\",\\n    \\\"mallic-acid\\\",\\n    \\\"alcalinity\\\",\\n    \\\"ash\\\",\\n    \\\"magnesium\\\",\\n    \\\"total-phenols\\\",\\n    \\\"flavonids\\\",\\n    \\\"nonflavonid-phenols\\\",\\n    \\\"proanthocyanins\\\",\\n    \\\"color-intensity\\\",\\n    \\\"hue\\\",\\n    \\\"od-of-diluted-wines\\\",\\n    \\\"proline\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    # replace 5more in doors to 5\\n    # df_car.loc[df_car['doors'] == '5more', 'doors'] = '5'\\n    # df_car[\\\"doors\\\"] = pd.to_numeric(df_car[\\\"doors\\\"])\\n    # replace more in passengers to 5\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore-print-color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p-index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\\n\\n\\n\\\"\\\"\\\"\\nhttps://www.alldatascience.com/classification/wine-dataset-analysis-with-python/\\n1:length -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_wine():\\n    df_wine = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\\",\\n        header=None,\\n        names=cols_wine,\\n    )\\n    # index mappings\\n    # length = len(df_wine.columns)\\n    # X = df_wine.loc[:, 1:].values\\n    # y = df_wine.loc[:, 0].values\\n    return df_wine\";\n",
       "                var nbb_formatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p-index\\\",\\n    \\\"labels\\\",\\n]\\n\\ncols_wine = [\\n    \\\"labels\\\",\\n    \\\"alcohol\\\",\\n    \\\"mallic-acid\\\",\\n    \\\"alcalinity\\\",\\n    \\\"ash\\\",\\n    \\\"magnesium\\\",\\n    \\\"total-phenols\\\",\\n    \\\"flavonids\\\",\\n    \\\"nonflavonid-phenols\\\",\\n    \\\"proanthocyanins\\\",\\n    \\\"color-intensity\\\",\\n    \\\"hue\\\",\\n    \\\"od-of-diluted-wines\\\",\\n    \\\"proline\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    # replace 5more in doors to 5\\n    # df_car.loc[df_car['doors'] == '5more', 'doors'] = '5'\\n    # df_car[\\\"doors\\\"] = pd.to_numeric(df_car[\\\"doors\\\"])\\n    # replace more in passengers to 5\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore-print-color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p-index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\\n\\n\\n\\\"\\\"\\\"\\nhttps://www.alldatascience.com/classification/wine-dataset-analysis-with-python/\\n1:length -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_wine():\\n    df_wine = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\\",\\n        header=None,\\n        names=cols_wine,\\n    )\\n    # index mappings\\n    # length = len(df_wine.columns)\\n    # X = df_wine.loc[:, 1:].values\\n    # y = df_wine.loc[:, 0].values\\n    return df_wine\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utility functions\n",
    "\n",
    "cols_mushroom = [\n",
    "    \"labels\",\n",
    "    \"cap-shape\",\n",
    "    \"cap-surface\",\n",
    "    \"cap-color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill-attachment\",\n",
    "    \"gill-spacing\",\n",
    "    \"gill-size\",\n",
    "    \"gill-color\",\n",
    "    \"stalk-shape\",\n",
    "    \"stalk-root\",\n",
    "    \"stalk-surface-above-ring\",\n",
    "    \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\",\n",
    "    \"stalk-color-below-ring\",\n",
    "    \"veil-type\",\n",
    "    \"veil-color\",\n",
    "    \"ring-number\",\n",
    "    \"ring-type\",\n",
    "    \"spore-print-color\",\n",
    "    \"population\",\n",
    "    \"habitat\",\n",
    "]\n",
    "\n",
    "cols_car = [\"buying\", \"maintenance\", \"doors\", \"passengers\", \"boot\", \"safety\", \"labels\"]\n",
    "\n",
    "cols_audiology = [\n",
    "    \"age_gt_60\",\n",
    "    \"air\",\n",
    "    \"airBoneGap\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bone\",\n",
    "    \"boneAbnormal\",\n",
    "    \"bser\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"speech\",\n",
    "    \"static_normal\",\n",
    "    \"tymp\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "    \"p-index\",\n",
    "    \"labels\",\n",
    "]\n",
    "\n",
    "cols_wine = [\n",
    "    \"labels\",\n",
    "    \"alcohol\",\n",
    "    \"mallic-acid\",\n",
    "    \"alcalinity\",\n",
    "    \"ash\",\n",
    "    \"magnesium\",\n",
    "    \"total-phenols\",\n",
    "    \"flavonids\",\n",
    "    \"nonflavonid-phenols\",\n",
    "    \"proanthocyanins\",\n",
    "    \"color-intensity\",\n",
    "    \"hue\",\n",
    "    \"od-of-diluted-wines\",\n",
    "    \"proline\",\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "0-5 -> data\n",
    "6 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_car():\n",
    "    df_car = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "        header=None,\n",
    "        names=cols_car,\n",
    "    )\n",
    "    # mappings using indexes:\n",
    "    # X = df_car.loc[:, :5].values\n",
    "    # y = df_car.loc[:, 6].values\n",
    "    labels_col = df_car.pop(\"labels\")\n",
    "    df_car.insert(0, \"labels\", labels_col)\n",
    "    return df_car\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "1-22 -> data\n",
    "0 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_mushroom():\n",
    "    df_mushroom = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "        header=None,\n",
    "        names=cols_mushroom,\n",
    "    )\n",
    "    # index mappings\n",
    "    # X = df_mushroom.loc[:, 1:].values\n",
    "    # y = df_mushroom.loc[:, 0].values\n",
    "    # drop values corelating a bit too much like this\n",
    "    df_mushroom = df_mushroom.drop(\"odor\", axis=1)\n",
    "    df_mushroom = df_mushroom.drop(\"spore-print-color\", axis=1)\n",
    "    return df_mushroom\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\n",
    "0:length-2 -> data\n",
    "length-1 unique id (p1-p200)\n",
    "length -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_audiology():\n",
    "    df_audiology = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\",\n",
    "        header=None,\n",
    "        names=cols_audiology,\n",
    "    )\n",
    "    # index mapping\n",
    "    # length = len(df_audiology.columns)\n",
    "    # X = df_audiology.loc[:, : length - 3].values\n",
    "    # y = df_audiology.loc[:, length - 1].values\n",
    "    df_audiology = df_audiology.drop(\"p-index\", axis=1)\n",
    "    labels_col = df_audiology.pop(\"labels\")\n",
    "    df_audiology.insert(0, \"labels\", labels_col)\n",
    "    return df_audiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e3ea44b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   labels                    8124 non-null   object\n",
      " 1   cap-shape                 8124 non-null   object\n",
      " 2   cap-surface               8124 non-null   object\n",
      " 3   cap-color                 8124 non-null   object\n",
      " 4   bruises                   8124 non-null   object\n",
      " 5   gill-attachment           8124 non-null   object\n",
      " 6   gill-spacing              8124 non-null   object\n",
      " 7   gill-size                 8124 non-null   object\n",
      " 8   gill-color                8124 non-null   object\n",
      " 9   stalk-shape               8124 non-null   object\n",
      " 10  stalk-root                8124 non-null   object\n",
      " 11  stalk-surface-above-ring  8124 non-null   object\n",
      " 12  stalk-surface-below-ring  8124 non-null   object\n",
      " 13  stalk-color-above-ring    8124 non-null   object\n",
      " 14  stalk-color-below-ring    8124 non-null   object\n",
      " 15  veil-type                 8124 non-null   object\n",
      " 16  veil-color                8124 non-null   object\n",
      " 17  ring-number               8124 non-null   object\n",
      " 18  ring-type                 8124 non-null   object\n",
      " 19  population                8124 non-null   object\n",
      " 20  habitat                   8124 non-null   object\n",
      "dtypes: object(21)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "Size of X:  (8124, 20)\n",
      "Size of y:  (8124,)\n",
      "Size of dataset costs:  (1, 20)\n",
      "Cost of classification on full dataset:  110509\n",
      "Data has been split.\n",
      "Labels encoded:  (6499,) ,  (1625,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_formatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "# dataset = load_car()\n",
    "dataset = load_mushroom()\n",
    "# dataset = load_audiology()\n",
    "\n",
    "print(dataset.info())\n",
    "# print(\"First five records:\")\n",
    "# print(dataset.head())\n",
    "\n",
    "# Extract to X and y\n",
    "X_cat = dataset.loc[:, dataset.columns != \"labels\"]\n",
    "y_cat = dataset.loc[:, \"labels\"]\n",
    "\n",
    "print(\"Size of X: \", np.shape(X_cat))\n",
    "print(\"Size of y: \", np.shape(y_cat))\n",
    "\n",
    "# Generate a matrix of costs\n",
    "max_cost_allowed = 10000\n",
    "\n",
    "dataset_costs = pd.DataFrame(\n",
    "    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\n",
    "    columns=X_cat.columns,\n",
    ")\n",
    "\n",
    "print(\"Size of dataset costs: \", np.shape(dataset_costs))\n",
    "print(\"Cost of classification on full dataset: \", dataset_costs.sum(axis=1)[0])\n",
    "\n",
    "max_seed_val = 2 ** 32 - 1\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\n",
    ")\n",
    "print(\"Data has been split.\")\n",
    "# print(\"X contains features: \", X_train.columns == \"index\")\n",
    "\n",
    "# Transform y using label encoder\n",
    "le = LabelEncoder().fit(y_cat)\n",
    "encoded_y_train = le.transform(y_train)\n",
    "encoded_y_test = le.transform(y_test)\n",
    "print(\"Labels encoded: \", np.shape(encoded_y_train), \", \", np.shape(encoded_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba83093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols created.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_formatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap-shape\\\",\\n    \\\"cap-surface\\\",\\n    \\\"cap-color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill-attachment\\\",\\n    \\\"gill-spacing\\\",\\n    \\\"gill-size\\\",\\n    \\\"gill-color\\\",\\n    \\\"stalk-shape\\\",\\n    \\\"stalk-root\\\",\\n    \\\"stalk-surface-above-ring\\\",\\n    \\\"stalk-surface-below-ring\\\",\\n    \\\"stalk-color-above-ring\\\",\\n    \\\"stalk-color-below-ring\\\",\\n    \\\"veil-type\\\",\\n    \\\"veil-color\\\",\\n    \\\"ring-number\\\",\\n    \\\"ring-type\\\",\\n    \\\"spore-print-color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collectors of values\n",
    "\n",
    "cols_one_hot = [\n",
    "    \"cap-shape\",\n",
    "    \"cap-surface\",\n",
    "    \"cap-color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill-attachment\",\n",
    "    \"gill-spacing\",\n",
    "    \"gill-size\",\n",
    "    \"gill-color\",\n",
    "    \"stalk-shape\",\n",
    "    \"stalk-root\",\n",
    "    \"stalk-surface-above-ring\",\n",
    "    \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\",\n",
    "    \"stalk-color-below-ring\",\n",
    "    \"veil-type\",\n",
    "    \"veil-color\",\n",
    "    \"ring-number\",\n",
    "    \"ring-type\",\n",
    "    \"spore-print-color\",\n",
    "    \"habitat\",\n",
    "    \"age_gt_60\",\n",
    "    \"airBoneGap\",\n",
    "    \"boneAbnormal\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"static_normal\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "]\n",
    "\n",
    "cols_ordinal = [\n",
    "    \"buying\",\n",
    "    \"maintenance\",\n",
    "    \"doors\",\n",
    "    \"passengers\",\n",
    "    \"boot\",\n",
    "    \"safety\",\n",
    "    \"population\",\n",
    "    \"air\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bser\",\n",
    "    \"bone\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"speech\",\n",
    "    \"tymp\",\n",
    "]\n",
    "\n",
    "print(\"Cols created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6bfe05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order created.\n",
      "    buying maintenance    doors passengers     boot   safety population  \\\n",
      "0      low         low        2          2    small      low          y   \n",
      "1      med         med        3          4      med      med          v   \n",
      "2     high        high        4       more      big     high          s   \n",
      "3    vhigh       vhigh    5more    filler1  filler1  filler1          n   \n",
      "4  filler1     filler1  filler1    filler2  filler2  filler2          c   \n",
      "5  filler2     filler2  filler2    filler3  filler3  filler3          a   \n",
      "6  filler3     filler3  filler3    filler4  filler4  filler4    filler1   \n",
      "\n",
      "        air      ar_c      ar_u      bser        bone    o_ar_c    o_ar_u  \\\n",
      "0    normal         ?         ?         ?           ?         ?         ?   \n",
      "1      mild    absent    absent    normal  unmeasured    absent    absent   \n",
      "2  moderate    normal    normal  degraded      normal    normal    normal   \n",
      "3    severe  elevated  elevated   filler1        mild  elevated  elevated   \n",
      "4  profound   filler1   filler1   filler2    moderate   filler1   filler1   \n",
      "5   filler1   filler2   filler2   filler3     filler1   filler2   filler2   \n",
      "6   filler2   filler3   filler3   filler4     filler3   filler3   filler3   \n",
      "\n",
      "       speech     tymp  \n",
      "0           ?        a  \n",
      "1  unmeasured       as  \n",
      "2   very_poor        b  \n",
      "3        poor       ad  \n",
      "4      normal        c  \n",
      "5        good  filler1  \n",
      "6   very_good  filler2  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_formatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make order of categories per each column in ordinal_columns\n",
    "order_of_ordinal_categories = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"buying\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"maintenance\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"doors\": [\"2\", \"3\", \"4\", \"5more\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"passengers\": [\"2\", \"4\", \"more\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"boot\": [\"small\", \"med\", \"big\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"safety\": [\"low\", \"med\", \"high\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"population\": [\"y\", \"v\", \"s\", \"n\", \"c\", \"a\", \"filler1\"],\n",
    "        \"air\": [\n",
    "            \"normal\",\n",
    "            \"mild\",\n",
    "            \"moderate\",\n",
    "            \"severe\",\n",
    "            \"profound\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "        ],\n",
    "        \"ar_c\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"ar_u\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"bser\": [\"?\", \"normal\", \"degraded\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"bone\": [\"?\", \"unmeasured\", \"normal\", \"mild\", \"moderate\", \"filler1\", \"filler3\"],\n",
    "        \"o_ar_c\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"o_ar_u\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"speech\": [\n",
    "            \"?\",\n",
    "            \"unmeasured\",\n",
    "            \"very_poor\",\n",
    "            \"poor\",\n",
    "            \"normal\",\n",
    "            \"good\",\n",
    "            \"very_good\",\n",
    "        ],\n",
    "        \"tymp\": [\"a\", \"as\", \"b\", \"ad\", \"c\", \"filler1\", \"filler2\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Order created.\")\n",
    "print(order_of_ordinal_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3c4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class EncodingCategoricalBayes has been created\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_formatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create custom encoding categorical bayes classifier\n",
    "class EncodingCategoricalBayes:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # classifier,\n",
    "        ordinal_categories_order,\n",
    "        ordinal_columns,\n",
    "        one_hot_columns,\n",
    "        dataset,\n",
    "    ):\n",
    "        # self.classifier = classifier\n",
    "        self.ordinal_categories_order = ordinal_categories_order\n",
    "        self.ordinal_columns = ordinal_columns\n",
    "        self.one_hot_columns = one_hot_columns\n",
    "        self.transformer_dataset = dataset\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier = CategoricalNB(min_categories=X.shape[0])\n",
    "        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\n",
    "        self.column_transformer.fit(X)\n",
    "        return self.classifier.fit(self.encode_features(X), y)\n",
    "        # return self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(self.encode_features(X))\n",
    "        # return self.classifier.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.classifier.predict_proba(self.encode_features(X))\n",
    "        # return self.classifier.predict_proba(X)\n",
    "\n",
    "    def encode_features(self, X):\n",
    "        encoded_X = self.column_transformer.transform(X)\n",
    "        if scipy.sparse.issparse(encoded_X):\n",
    "            encoded_X = encoded_X.toarray()\n",
    "        return encoded_X\n",
    "\n",
    "    def make_column_transformer(self, X):\n",
    "        # Get current ordinal and one hot columns\n",
    "        total_column_list = X.select_dtypes(include=\"object\").columns\n",
    "        # print(\"Total col list: \", total_column_list)\n",
    "        current_columns_one_hot = self.collect_current_one_hot_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "        current_columns_ordinal = self.collect_current_ordinal_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "\n",
    "        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\n",
    "            current_columns_ordinal\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"Columns in column transformer: \",\n",
    "            current_columns_one_hot,\n",
    "            \" and \",\n",
    "            current_columns_ordinal,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Create column transformer\n",
    "        column_transformer = make_column_transformer(\n",
    "            (OneHotEncoder(), current_columns_one_hot),\n",
    "            (\n",
    "                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\n",
    "                current_columns_ordinal,\n",
    "            ),\n",
    "        )\n",
    "        return column_transformer\n",
    "\n",
    "    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\n",
    "        # Get common cols to feed them in proper order to ordinal encoder\n",
    "        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\n",
    "            argColumns\n",
    "        )\n",
    "        # Convert to list\n",
    "        order_of_ordinal_categories_list = (\n",
    "            self.ordinal_categories_order[index_of_common_cols]\n",
    "            .values.transpose()\n",
    "            .tolist()\n",
    "        )\n",
    "        return order_of_ordinal_categories_list\n",
    "\n",
    "    def intersection(self, lst1, lst2):\n",
    "        # collects common elements in both lists\n",
    "        return [value for value in lst1 if value in lst2]\n",
    "\n",
    "    def collect_current_one_hot_columns(self, argCols):\n",
    "        return self.intersection(self.one_hot_columns, argCols)\n",
    "\n",
    "    def collect_current_ordinal_columns(self, argCols):\n",
    "        # make list of all values and create steps for them\n",
    "        return self.intersection(self.ordinal_columns, argCols)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.classifier.get_params()\n",
    "\n",
    "\n",
    "print(\"Class EncodingCategoricalBayes has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd1b940f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Forward Feature Selector is created.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n    ):\\n        # Make copies as we'll be altering these datasets\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        # Variables\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n        duplicates = pd.DataFrame()\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # Statistics\\n        loop_number = 0\\n        total_number_of_cases = np.shape(X_tst)[0]\\n        classified_size = 0\\n\\n        # Main loop, until all test classes are classified\\n        while unused_features and not X_tst.empty:\\n            # make a list of features with their predicted accuracy\\n            print(\\\"Start of loop number: \\\", loop_number)\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            print(\\\"Picked feature: \\\", best_next_feature)\\n            print(\\\"Current feature set: \\\", current_features)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas = self.predict_proba_wrapper(\\n                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates\\n            )\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost_of_classification\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            current_features_string = \\\",\\\".join(map(str, current_features))\\n\\n            feature_series = current_features_string * np.shape(X_tst)[0]\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"feature_used_to_classify\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_probas[\\\"highest_proba\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n\\n            batch_result_dataframe = pd.concat(\\n                [\\n                    outcomes,\\n                    highest_probas,\\n                    cost_of_classification,\\n                    feature_used_to_classify,\\n                ],\\n                axis=1,\\n            )\\n\\n            # print(\\\"Final result dataframe:\\\")\\n            # print(batch_result_dataframe)\\n            # print(\\\"Are X test and result dataframe indexes equal? :\\\",X_tst.index.equals(batch_result_dataframe.index))\\n\\n            rows_classified = highest_probas.loc[condition].index\\n            rows_unclassified = X_tst.index.difference(rows_classified)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n\\n            # print(\\\"Rows classified: \\\", rows_classified)\\n            # print(\\\"Rows unclassified: \\\", rows_unclassified)\\n            # print(\\\"X test before:\\\")\\n            # print(X_tst)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows_classified)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(rows_unclassified)[0])\\n\\n            X_tst.drop(rows_classified, inplace=True)\\n            y_tst.drop(rows_classified, inplace=True)\\n            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\\n            if unused_features:\\n                batch_result_dataframe.drop(rows_unclassified, inplace=True)\\n            # print(\\\"Are X test and result dataframe indexes unequal? :\\\",not X_tst.index.equals(batch_result_dataframe.index),)\\n\\n            # print(\\\"final: \\\")\\n            # print(batch_result_dataframe)\\n            final_result_dataframe = pd.concat(\\n                [final_result_dataframe, batch_result_dataframe]\\n            )\\n\\n            # percentage classified\\n            classified_size += rows_classified.size\\n            print(\\\"End of loop \\\", loop_number)\\n            loop_number += 1\\n            print(\\n                \\\"Classified classes: \\\",\\n                classified_size,\\n                \\\"/\\\",\\n                total_number_of_cases,\\n                classified_size / total_number_of_cases * 100,\\n                \\\"%\\\",\\n            )\\n            # now go back to the beginning of the loop and check for unclassified classes\\n\\n        print(\\\"Out of the loop. Usable features ran out, or no more cases to classify.\\\")\\n        return final_result_dataframe\\n\\n    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"outcome\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_proba\\\"])\\n        for index, row_entry in X_test.iterrows():\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                # X_train, y_train, duplicates[index]\\n                X_train,\\n                y_train,\\n                duplicates,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"outcome\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_proba\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn\\n\\n    def prepare_train_dataset(self, X_train_arg, y_train_arg, duplicates_per_case):\\n        X_train = copy.deepcopy(X_train_arg)\\n        y_train = copy.deepcopy(y_train_arg)\\n\\n        if duplicates_per_case.empty:\\n            return X_train, y_train\\n\\n        \\\"\\\"\\\"\\n        we take first feature and duplicate the rows with that feature\\n        then another (together with the previously duplicated features)\\n        \\\"\\\"\\\"\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = X_train.insert(y_train)\\n        # for each feature\\n        for col_name in duplicates_per_case.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            print(\\\"Query:\\\", query)\\n            print(full_test_data.query(query))\\n            return null\\n            full_test_data.append(full_test_data.query(query), ignore_index=True)\\n\\n        \\\"\\\"\\\"\\n        # we can also easily exclude the appended features like below\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\\n        print(duplicate_rows)\\n        full_test_data.append(duplicate_rows, ignore_index=True)\\n        \\\"\\\"\\\"\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\";\n",
       "                var nbb_formatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n    ):\\n        # Make copies as we'll be altering these datasets\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        # Variables\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n        duplicates = pd.DataFrame()\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # Statistics\\n        loop_number = 0\\n        total_number_of_cases = np.shape(X_tst)[0]\\n        classified_size = 0\\n\\n        # Main loop, until all test classes are classified\\n        while unused_features and not X_tst.empty:\\n            # make a list of features with their predicted accuracy\\n            print(\\\"Start of loop number: \\\", loop_number)\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            print(\\\"Picked feature: \\\", best_next_feature)\\n            print(\\\"Current feature set: \\\", current_features)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas = self.predict_proba_wrapper(\\n                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates\\n            )\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost_of_classification\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            current_features_string = \\\",\\\".join(map(str, current_features))\\n\\n            feature_series = current_features_string * np.shape(X_tst)[0]\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"feature_used_to_classify\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_probas[\\\"highest_proba\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n\\n            batch_result_dataframe = pd.concat(\\n                [\\n                    outcomes,\\n                    highest_probas,\\n                    cost_of_classification,\\n                    feature_used_to_classify,\\n                ],\\n                axis=1,\\n            )\\n\\n            # print(\\\"Final result dataframe:\\\")\\n            # print(batch_result_dataframe)\\n            # print(\\\"Are X test and result dataframe indexes equal? :\\\",X_tst.index.equals(batch_result_dataframe.index))\\n\\n            rows_classified = highest_probas.loc[condition].index\\n            rows_unclassified = X_tst.index.difference(rows_classified)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n\\n            # print(\\\"Rows classified: \\\", rows_classified)\\n            # print(\\\"Rows unclassified: \\\", rows_unclassified)\\n            # print(\\\"X test before:\\\")\\n            # print(X_tst)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows_classified)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(rows_unclassified)[0])\\n\\n            X_tst.drop(rows_classified, inplace=True)\\n            y_tst.drop(rows_classified, inplace=True)\\n            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\\n            if unused_features:\\n                batch_result_dataframe.drop(rows_unclassified, inplace=True)\\n            # print(\\\"Are X test and result dataframe indexes unequal? :\\\",not X_tst.index.equals(batch_result_dataframe.index),)\\n\\n            # print(\\\"final: \\\")\\n            # print(batch_result_dataframe)\\n            final_result_dataframe = pd.concat(\\n                [final_result_dataframe, batch_result_dataframe]\\n            )\\n\\n            # percentage classified\\n            classified_size += rows_classified.size\\n            print(\\\"End of loop \\\", loop_number)\\n            loop_number += 1\\n            print(\\n                \\\"Classified classes: \\\",\\n                classified_size,\\n                \\\"/\\\",\\n                total_number_of_cases,\\n                classified_size / total_number_of_cases * 100,\\n                \\\"%\\\",\\n            )\\n            # now go back to the beginning of the loop and check for unclassified classes\\n\\n        print(\\\"Out of the loop. Usable features ran out, or no more cases to classify.\\\")\\n        return final_result_dataframe\\n\\n    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"outcome\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_proba\\\"])\\n        for index, row_entry in X_test.iterrows():\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                # X_train, y_train, duplicates[index]\\n                X_train,\\n                y_train,\\n                duplicates,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"outcome\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_proba\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn\\n\\n    def prepare_train_dataset(self, X_train_arg, y_train_arg, duplicates_per_case):\\n        X_train = copy.deepcopy(X_train_arg)\\n        y_train = copy.deepcopy(y_train_arg)\\n\\n        if duplicates_per_case.empty:\\n            return X_train, y_train\\n\\n        \\\"\\\"\\\"\\n        we take first feature and duplicate the rows with that feature\\n        then another (together with the previously duplicated features)\\n        \\\"\\\"\\\"\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = X_train.insert(y_train)\\n        # for each feature\\n        for col_name in duplicates_per_case.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            print(\\\"Query:\\\", query)\\n            print(full_test_data.query(query))\\n            return null\\n            full_test_data.append(full_test_data.query(query), ignore_index=True)\\n\\n        \\\"\\\"\\\"\\n        # we can also easily exclude the appended features like below\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates.columns.tolist():\\n            # create query\\n            query = col_name + \\\"=\\\" + str(full_test_data[col_name][0])\\n            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\\n        print(duplicate_rows)\\n        full_test_data.append(duplicate_rows, ignore_index=True)\\n        \\\"\\\"\\\"\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sequential Forward Feature Selector\n",
    "class SequentialForwardFeatureSelector:\n",
    "    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\n",
    "        self.classification_costs = classification_costs\n",
    "        self.CV_folds = CV_folds\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "\n",
    "    def sequential_predict(\n",
    "        self,\n",
    "        X_train_original,\n",
    "        y_train_original,\n",
    "        X_test_original,\n",
    "        y_test_original,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "        whole_dataset,\n",
    "        data_duplication_flag\n",
    "    ):\n",
    "        # Make copies as we'll be altering these datasets\n",
    "        X_tr = copy.deepcopy(X_train_original)\n",
    "        y_tr = copy.deepcopy(y_train_original)\n",
    "        X_tst = copy.deepcopy(X_test_original)\n",
    "        y_tst = copy.deepcopy(y_test_original)\n",
    "\n",
    "        # Variables\n",
    "        final_result_columns = [\n",
    "            \"highest_proba\",\n",
    "            \"outcome\",\n",
    "            \"cost_of_classification\",\n",
    "            \"feature_used_to_classify\",\n",
    "        ]\n",
    "        duplicates = pd.DataFrame()\n",
    "        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\n",
    "        unused_features = X_tst.columns.tolist()\n",
    "        current_features = []\n",
    "\n",
    "        # Statistics\n",
    "        loop_number = 0\n",
    "        total_number_of_cases = np.shape(X_tst)[0]\n",
    "        classified_size = 0\n",
    "\n",
    "        # Main loop, until all test classes are classified\n",
    "        while unused_features and not X_tst.empty:\n",
    "            # make a list of features with their predicted accuracy\n",
    "            print(\"Start of loop number: \", loop_number)\n",
    "            accuracy_per_new_feature = self.find_next_best_feature(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                unused_features,\n",
    "                current_features,\n",
    "                whole_dataset,\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "            )\n",
    "\n",
    "            # we have all features and their accuracies, we pick the best one\n",
    "            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\n",
    "            # and adjust feature trackers\n",
    "            unused_features.remove(best_next_feature)\n",
    "            current_features.append(best_next_feature)\n",
    "\n",
    "            print(\"Picked feature: \", best_next_feature)\n",
    "            print(\"Current feature set: \", current_features)\n",
    "\n",
    "            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\n",
    "\n",
    "            # create X_train subset with apropriate features\n",
    "            X_train_subset_full = pd.DataFrame(X_tr[current_features])\n",
    "            X_test_subset_full = pd.DataFrame(X_tst[current_features])\n",
    "            # make new classifier (due to different encoder data)\n",
    "            classifier = self.make_encoding_categorical_bayes(\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "                pd.DataFrame(dataset[current_features]),\n",
    "            )\n",
    "            # define duplicates before end of refactorization and moving forward\n",
    "            outcomes, highest_probas = self.predict_proba_wrapper(\n",
    "                classifier, X_train_subset_full, y_train, X_test_subset_full, duplicates, data_duplication_flag\n",
    "            )\n",
    "\n",
    "            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\n",
    "\n",
    "            cost_series = (\n",
    "                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\n",
    "            )  # remember to change to list of featureS\n",
    "            cost_of_classification = pd.DataFrame(\n",
    "                cost_series, columns=[\"cost_of_classification\"], index=X_tst.index\n",
    "            )\n",
    "            # print(cost_of_classification)\n",
    "\n",
    "            current_features_string = \",\".join(map(str, current_features))\n",
    "\n",
    "            feature_series = current_features_string * np.shape(X_tst)[0]\n",
    "            feature_used_to_classify = pd.DataFrame(\n",
    "                feature_series, columns=[\"feature_used_to_classify\"], index=X_tst.index\n",
    "            )\n",
    "            # print(feature_used_to_classify)\n",
    "\n",
    "            # remove already classified classes\n",
    "            condition = highest_probas[\"highest_proba\"] > (\n",
    "                1 - self.uncertainty_threshold\n",
    "            )\n",
    "\n",
    "            batch_result_dataframe = pd.concat(\n",
    "                [\n",
    "                    outcomes,\n",
    "                    highest_probas,\n",
    "                    cost_of_classification,\n",
    "                    feature_used_to_classify,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            # print(\"Final result dataframe:\")\n",
    "            # print(batch_result_dataframe)\n",
    "            # print(\"Are X test and result dataframe indexes equal? :\",X_tst.index.equals(batch_result_dataframe.index))\n",
    "\n",
    "            rows_classified = highest_probas.loc[condition].index\n",
    "            rows_unclassified = X_tst.index.difference(rows_classified)\n",
    "            y_tst = pd.DataFrame(y_tst, columns=[\"labels\"], index=X_tst.index)\n",
    "\n",
    "            # print(\"Rows classified: \", rows_classified)\n",
    "            # print(\"Rows unclassified: \", rows_unclassified)\n",
    "            # print(\"X test before:\")\n",
    "            # print(X_tst)\n",
    "            # print(\"Number of rows to drop:\", np.shape(rows_classified)[0])\n",
    "            # print(\"Number of rows to leave:\", np.shape(rows_unclassified)[0])\n",
    "\n",
    "            X_tst.drop(rows_classified, inplace=True)\n",
    "            y_tst.drop(rows_classified, inplace=True)\n",
    "            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\n",
    "            if unused_features:\n",
    "                batch_result_dataframe.drop(rows_unclassified, inplace=True)\n",
    "            # print(\"Are X test and result dataframe indexes unequal? :\",not X_tst.index.equals(batch_result_dataframe.index),)\n",
    "\n",
    "            # print(\"final: \")\n",
    "            # print(batch_result_dataframe)\n",
    "            final_result_dataframe = pd.concat(\n",
    "                [final_result_dataframe, batch_result_dataframe]\n",
    "            )\n",
    "\n",
    "            # percentage classified\n",
    "            classified_size += rows_classified.size\n",
    "            print(\"End of loop \", loop_number)\n",
    "            loop_number += 1\n",
    "            print(\n",
    "                \"Classified classes: \",\n",
    "                classified_size,\n",
    "                \"/\",\n",
    "                total_number_of_cases,\n",
    "                classified_size / total_number_of_cases * 100,\n",
    "                \"%\",\n",
    "            )\n",
    "            # now go back to the beginning of the loop and check for unclassified classes\n",
    "\n",
    "        print(\"Out of the loop. Usable features ran out, or no more cases to classify.\")\n",
    "        return final_result_dataframe\n",
    "\n",
    "    def predict_proba_wrapper(self, classifier, X_train, y_train, X_test, duplicates, flag):\n",
    "        outcomes_fn = pd.DataFrame(columns=[\"outcome\"])\n",
    "        highest_probas_fn = pd.DataFrame(columns=[\"highest_proba\"])\n",
    "        for index, row_entry in X_test.iterrows():\n",
    "            X_train_mod, y_train_mod = self.prepare_train_dataset(\n",
    "                # X_train, y_train, duplicates[index]\n",
    "                X_train,\n",
    "                y_train,\n",
    "                duplicates,\n",
    "                falg\n",
    "            )\n",
    "            classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "            df_row_entry = row_entry.to_frame().T\n",
    "\n",
    "            # gotta make it in 2 steps bc of no column name tracking in numpy\n",
    "            new_outcome_df = pd.DataFrame(\n",
    "                classifier.predict(df_row_entry),\n",
    "                columns=[\"outcome\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "\n",
    "            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\n",
    "\n",
    "            probas_fn = classifier.predict_proba(df_row_entry)\n",
    "            new_probas_df = pd.DataFrame(\n",
    "                np.max(np.max(probas_fn, axis=1), axis=0),\n",
    "                columns=[\"highest_proba\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\n",
    "        return outcomes_fn, highest_probas_fn\n",
    "\n",
    "    def prepare_train_dataset(self, X_train_arg, y_train_arg, duplicates_per_case, flag):\n",
    "        X_train = copy.deepcopy(X_train_arg)\n",
    "        y_train = copy.deepcopy(y_train_arg)\n",
    "\n",
    "        if duplicates_per_case.empty or flag:\n",
    "            return X_train, y_train\n",
    "\n",
    "        \"\"\"\n",
    "        we take first feature and duplicate the rows with that feature\n",
    "        then another (together with the previously duplicated features)\n",
    "        \"\"\"\n",
    "\n",
    "        # make 1 full dataset for easy modification\n",
    "        full_test_data = X_train.insert(y_train)\n",
    "        # for each feature\n",
    "        for col_name in duplicates_per_case.columns.tolist():\n",
    "            # create query\n",
    "            query = col_name + \"=\" + str(full_test_data[col_name][0])\n",
    "            print(\"Query:\", query)\n",
    "            print(full_test_data.query(query))\n",
    "            return null\n",
    "            full_test_data.append(full_test_data.query(query), ignore_index=True)\n",
    "\n",
    "        \"\"\"\n",
    "        # we can also easily exclude the appended features like below\n",
    "        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\n",
    "        for col_name in duplicates.columns.tolist():\n",
    "            # create query\n",
    "            query = col_name + \"=\" + str(full_test_data[col_name][0])\n",
    "            duplicate_rows.append(full_test_data.query(query), ignore_index=True)\n",
    "        print(duplicate_rows)\n",
    "        full_test_data.append(duplicate_rows, ignore_index=True)\n",
    "        \"\"\"\n",
    "        # return X_train, y_train\n",
    "        return (\n",
    "            full_test_data.loc[:, full_test_data.columns != \"labels\"],\n",
    "            full_test_data.loc[:, \"labels\"],\n",
    "        )\n",
    "\n",
    "    def find_next_best_feature(\n",
    "        self,\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        unused_feat,\n",
    "        current_feat,\n",
    "        whole_dataset,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "    ):\n",
    "        kf = KFold(n_splits=self.CV_folds)\n",
    "        accuracy_per_new_feature = pd.DataFrame(\n",
    "            0, index=np.arange(1), columns=unused_feat,\n",
    "        )\n",
    "        for new_feature in unused_feat:\n",
    "            # print(\"Calculating feature: \", new_feature)\n",
    "            sum_of_accuracies = 0\n",
    "            feature_set_to_try = copy.deepcopy(current_feat)\n",
    "            feature_set_to_try.append(new_feature)\n",
    "            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\n",
    "\n",
    "            for train_index, test_index in kf.split(X_tr):\n",
    "                # create _train, _cv_test, _test splits\n",
    "                # no need to reshuffle it, it's already in random order\n",
    "                # X is a dataframe\n",
    "                X_train, X_cv = (\n",
    "                    X_tr.iloc[train_index],\n",
    "                    X_tr.iloc[test_index],\n",
    "                )\n",
    "\n",
    "                # y is a numpy array\n",
    "                y_train, y_cv = (\n",
    "                    y_tr[train_index],\n",
    "                    y_tr[test_index],\n",
    "                )\n",
    "\n",
    "                # print(\"train: \", train_index, \"test: \", test_index)\n",
    "\n",
    "                # add feture to test to X\n",
    "                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\n",
    "                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\n",
    "\n",
    "                # make classifier\n",
    "                classifier = self.make_encoding_categorical_bayes(\n",
    "                    ordinal_categoires_order,\n",
    "                    cols_ordinal,\n",
    "                    cols_one_hot,\n",
    "                    dataset_for_encoder,\n",
    "                )\n",
    "\n",
    "                # train classifier\n",
    "                classifier.fit(X_train_subset, y_train)\n",
    "                # predict\n",
    "                y_cv_prediciton = classifier.predict(X_cv_subset)\n",
    "                # judge accuracy of new feature subset\n",
    "                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\n",
    "\n",
    "            # save accuracy per new feature\n",
    "            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\n",
    "            # print(\"Finished calculations for feature: \",new_feature,\"Accuracy: \",accuracy_per_new_feature[new_feature][0])\n",
    "\n",
    "        return accuracy_per_new_feature\n",
    "\n",
    "    def make_encoding_categorical_bayes(\n",
    "        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\n",
    "    ):\n",
    "        return EncodingCategoricalBayes(\n",
    "            # classifier=CategoricalNB(),\n",
    "            ordinal_categories_order=order_of_ordinal_categories,\n",
    "            ordinal_columns=cols_ordinal,\n",
    "            one_hot_columns=cols_one_hot,\n",
    "            dataset=whole_dataset,\n",
    "        )\n",
    "\n",
    "    def get_classification_costs(self, list_of_categories):\n",
    "        return self.classification_costs[\n",
    "            self.classification_costs.columns.intersection(list_of_categories)\n",
    "        ].sum(axis=1)[0]\n",
    "\n",
    "\n",
    "print(\"Sequential Forward Feature Selector is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36541769",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of loop number:  0\n",
      "Picked feature:  gill-color\n",
      "Current feature set:  ['gill-color']\n",
      "End of loop  0\n",
      "Classified classes:  656 / 1625 40.36923076923077 %\n",
      "Start of loop number:  1\n",
      "Picked feature:  stalk-color-below-ring\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring']\n",
      "End of loop  1\n",
      "Classified classes:  1087 / 1625 66.8923076923077 %\n",
      "Start of loop number:  2\n",
      "Picked feature:  stalk-shape\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape']\n",
      "End of loop  2\n",
      "Classified classes:  1087 / 1625 66.8923076923077 %\n",
      "Start of loop number:  3\n",
      "Picked feature:  veil-color\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color']\n",
      "End of loop  3\n",
      "Classified classes:  1087 / 1625 66.8923076923077 %\n",
      "Start of loop number:  4\n",
      "Picked feature:  gill-spacing\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing']\n",
      "End of loop  4\n",
      "Classified classes:  1253 / 1625 77.1076923076923 %\n",
      "Start of loop number:  5\n",
      "Picked feature:  stalk-surface-below-ring\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring']\n",
      "End of loop  5\n",
      "Classified classes:  1471 / 1625 90.52307692307693 %\n",
      "Start of loop number:  6\n",
      "Picked feature:  gill-size\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size']\n",
      "End of loop  6\n",
      "Classified classes:  1603 / 1625 98.64615384615385 %\n",
      "Start of loop number:  7\n",
      "Picked feature:  cap-surface\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface']\n",
      "End of loop  7\n",
      "Classified classes:  1603 / 1625 98.64615384615385 %\n",
      "Start of loop number:  8\n",
      "Picked feature:  ring-number\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number']\n",
      "End of loop  8\n",
      "Classified classes:  1612 / 1625 99.2 %\n",
      "Start of loop number:  9\n",
      "Picked feature:  veil-type\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type']\n",
      "End of loop  9\n",
      "Classified classes:  1612 / 1625 99.2 %\n",
      "Start of loop number:  10\n",
      "Picked feature:  population\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population']\n",
      "End of loop  10\n",
      "Classified classes:  1612 / 1625 99.2 %\n",
      "Start of loop number:  11\n",
      "Picked feature:  gill-attachment\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment']\n",
      "End of loop  11\n",
      "Classified classes:  1618 / 1625 99.56923076923077 %\n",
      "Start of loop number:  12\n",
      "Picked feature:  cap-shape\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment', 'cap-shape']\n",
      "End of loop  12\n",
      "Classified classes:  1618 / 1625 99.56923076923077 %\n",
      "Start of loop number:  13\n",
      "Picked feature:  cap-color\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment', 'cap-shape', 'cap-color']\n",
      "End of loop  13\n",
      "Classified classes:  1618 / 1625 99.56923076923077 %\n",
      "Start of loop number:  14\n",
      "Picked feature:  stalk-color-above-ring\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment', 'cap-shape', 'cap-color', 'stalk-color-above-ring']\n",
      "End of loop  14\n",
      "Classified classes:  1619 / 1625 99.63076923076923 %\n",
      "Start of loop number:  15\n",
      "Picked feature:  ring-type\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment', 'cap-shape', 'cap-color', 'stalk-color-above-ring', 'ring-type']\n",
      "End of loop  15\n",
      "Classified classes:  1622 / 1625 99.81538461538462 %\n",
      "Start of loop number:  16\n",
      "Picked feature:  stalk-surface-above-ring\n",
      "Current feature set:  ['gill-color', 'stalk-color-below-ring', 'stalk-shape', 'veil-color', 'gill-spacing', 'stalk-surface-below-ring', 'gill-size', 'cap-surface', 'ring-number', 'veil-type', 'population', 'gill-attachment', 'cap-shape', 'cap-color', 'stalk-color-above-ring', 'ring-type', 'stalk-surface-above-ring']\n",
      "End of loop  16\n",
      "Classified classes:  1625 / 1625 100.0 %\n",
      "Out of the loop. Usable features ran out, or no more cases to classify.\n",
      "      highest_proba outcome cost_of_classification  \\\n",
      "7996       0.989480       e                   7893   \n",
      "6350       0.999241       p                   7893   \n",
      "3302       0.952654       e                   7893   \n",
      "2073       0.952654       e                   7893   \n",
      "7587       0.999241       p                   7893   \n",
      "...             ...     ...                    ...   \n",
      "483        0.972193       e                  89761   \n",
      "3326       0.971520       e                  89761   \n",
      "1173       0.982553       e                  95275   \n",
      "4133       0.989565       e                  95275   \n",
      "547        0.979668       e                  95275   \n",
      "\n",
      "                               feature_used_to_classify  \n",
      "7996  gill-colorgill-colorgill-colorgill-colorgill-c...  \n",
      "6350  gill-colorgill-colorgill-colorgill-colorgill-c...  \n",
      "3302  gill-colorgill-colorgill-colorgill-colorgill-c...  \n",
      "2073  gill-colorgill-colorgill-colorgill-colorgill-c...  \n",
      "7587  gill-colorgill-colorgill-colorgill-colorgill-c...  \n",
      "...                                                 ...  \n",
      "483   gill-color,stalk-color-below-ring,stalk-shape,...  \n",
      "3326  gill-color,stalk-color-below-ring,stalk-shape,...  \n",
      "1173  gill-color,stalk-color-below-ring,stalk-shape,...  \n",
      "4133  gill-color,stalk-color-below-ring,stalk-shape,...  \n",
      "547   gill-color,stalk-color-below-ring,stalk-shape,...  \n",
      "\n",
      "[1625 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nresults = selector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\\n\\nprint(results)\";\n",
       "                var nbb_formatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nresults = selector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,\\n    cols_ordinal,\\n    cols_one_hot,\\n    X_cat,\\n)\\n\\nprint(results)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\n",
    "\n",
    "selector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\n",
    "\n",
    "# Train the model using the training sets\n",
    "results = selector.sequential_predict(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    X_test,\n",
    "    encoded_y_test,\n",
    "    order_of_ordinal_categories, # order of ordinal categories\n",
    "    cols_ordinal,# list of ordinal columns in whole data\n",
    "    cols_one_hot, # list of one hot columns in whole data\n",
    "    X_cat, # encoder dataset\n",
    "    True # feature duplication when classifying\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d835b50d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Accuracy: 88.18461538461538 %\n",
      "F1 score: 87.96783858964496 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"# Model Accuracy, how often is the classifier correct?\\nprint(type(y_test))\\ny_pred = results[\\\"outcome\\\"].sort_index()  # warning: not sorted!\\ny_test.sort_index(inplace=True)\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_formatted_code = \"# Model Accuracy, how often is the classifier correct?\\nprint(type(y_test))\\ny_pred = results[\\\"outcome\\\"].sort_index()  # warning: not sorted!\\ny_test.sort_index(inplace=True)\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "y_pred = results[\"outcome\"].sort_index()  # warning: not sorted!\n",
    "y_test.sort_index(inplace=True)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred) * 100, \"%\")\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred, average=\"weighted\") * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduct some useful metrics:\n",
    "# mean cost\n",
    "# median cost\n",
    "# difference in accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
