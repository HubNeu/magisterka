{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f0c540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (Temp/ipykernel_1480/1084367042.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Hubert\\AppData\\Local\\Temp/ipykernel_1480/1084367042.py\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    -sequential feature selection\u001b[0m\n\u001b[1;37m                                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solved:\n",
    "    -It's possible for train-test split to split data in such a way, that\n",
    "   after encoding, X_train and X_test have different numbers of features.\n",
    "   Split has to be rerun to fix it. First encode, then split again?\n",
    "   BUT IT STILL HAS TO BE ENCODED AND SPLIT BEFORE STARTING CROSS VALIDATION\n",
    "   AND SEQUENTIAL FEATURE SELECTION. MAYBE APPEND DURING SPLIT AND THEN SPLIT\n",
    "   AGAIN?\n",
    "    -Improve feature encoding to have proper ordering instead of random numbers\n",
    "    which currently influence classification accuracy:\n",
    "    https://datascience.stackexchange.com/questions/72343/encoding-with-ordinalencoder-how-to-give-levels-as-user-input\n",
    "\n",
    "Fishy:\n",
    "    -check and check for data leakage (def: https://scikit-learn.org/stable/glossary.html)\n",
    "\n",
    "TODO:\n",
    "    -add cost counting to SFS wrapper\n",
    "    -???\n",
    "    -tests and profit???\n",
    "    -report a bug with indexes when predicting X_test using audiology and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c628d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs imported. Python version is:  3.9.7\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\nimport numpy as np\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_formatted_code = \"## SKLEARN\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\\nfrom sklearn import metrics\\nfrom sklearn.compose import make_column_transformer\\nfrom sklearn.model_selection import KFold\\nimport numpy as np\\n\\nnp.set_printoptions(suppress=True)\\nimport pandas as pd\\nfrom platform import python_version\\nimport scipy\\nimport random\\nimport copy\\n\\n# works, sort of only. Possible additional commas that shouldn't be there.\\n%load_ext nb_black\\n\\nprint(\\\"Libs imported. Python version is: \\\", python_version())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## SKLEARN\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# works, sort of only. Possible additional commas that shouldn't be there.\n",
    "%load_ext nb_black\n",
    "\n",
    "print(\"Libs imported. Python version is: \", python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a810b851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap_shape\\\",\\n    \\\"cap_surface\\\",\\n    \\\"cap_color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill_attachment\\\",\\n    \\\"gill_spacing\\\",\\n    \\\"gill_size\\\",\\n    \\\"gill_color\\\",\\n    \\\"stalk_shape\\\",\\n    \\\"stalk_root\\\",\\n    \\\"stalk_surface_above_ring\\\",\\n    \\\"stalk_surface_below_ring\\\",\\n    \\\"stalk_color_above_ring\\\",\\n    \\\"stalk_color_below_ring\\\",\\n    \\\"veil_type\\\",\\n    \\\"veil_color\\\",\\n    \\\"ring_number\\\",\\n    \\\"ring_type\\\",\\n    \\\"spore_print_color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p_index\\\",\\n    \\\"labels\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    # drop values corelating a bit too much like this\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore_print_color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p_index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\";\n",
       "                var nbb_formatted_code = \"# utility functions\\n\\ncols_mushroom = [\\n    \\\"labels\\\",\\n    \\\"cap_shape\\\",\\n    \\\"cap_surface\\\",\\n    \\\"cap_color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill_attachment\\\",\\n    \\\"gill_spacing\\\",\\n    \\\"gill_size\\\",\\n    \\\"gill_color\\\",\\n    \\\"stalk_shape\\\",\\n    \\\"stalk_root\\\",\\n    \\\"stalk_surface_above_ring\\\",\\n    \\\"stalk_surface_below_ring\\\",\\n    \\\"stalk_color_above_ring\\\",\\n    \\\"stalk_color_below_ring\\\",\\n    \\\"veil_type\\\",\\n    \\\"veil_color\\\",\\n    \\\"ring_number\\\",\\n    \\\"ring_type\\\",\\n    \\\"spore_print_color\\\",\\n    \\\"population\\\",\\n    \\\"habitat\\\",\\n]\\n\\ncols_car = [\\\"buying\\\", \\\"maintenance\\\", \\\"doors\\\", \\\"passengers\\\", \\\"boot\\\", \\\"safety\\\", \\\"labels\\\"]\\n\\ncols_audiology = [\\n    \\\"age_gt_60\\\",\\n    \\\"air\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bone\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"bser\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"speech\\\",\\n    \\\"static_normal\\\",\\n    \\\"tymp\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n    \\\"p_index\\\",\\n    \\\"labels\\\",\\n]\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/car+evaluation\\n0-5 -> data\\n6 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_car():\\n    df_car = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\\\",\\n        header=None,\\n        names=cols_car,\\n    )\\n    # mappings using indexes:\\n    # X = df_car.loc[:, :5].values\\n    # y = df_car.loc[:, 6].values\\n    labels_col = df_car.pop(\\\"labels\\\")\\n    df_car.insert(0, \\\"labels\\\", labels_col)\\n    return df_car\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/mushroom\\n1-22 -> data\\n0 -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_mushroom():\\n    df_mushroom = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\\\",\\n        header=None,\\n        names=cols_mushroom,\\n    )\\n    # index mappings\\n    # X = df_mushroom.loc[:, 1:].values\\n    # y = df_mushroom.loc[:, 0].values\\n    # drop values corelating a bit too much like this\\n    df_mushroom = df_mushroom.drop(\\\"odor\\\", axis=1)\\n    df_mushroom = df_mushroom.drop(\\\"spore_print_color\\\", axis=1)\\n    return df_mushroom\\n\\n\\n\\\"\\\"\\\"\\nhttps://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\\n0:length-2 -> data\\nlength-1 unique id (p1-p200)\\nlength -> labels\\n\\\"\\\"\\\"\\n\\n\\ndef load_audiology():\\n    df_audiology = pd.read_csv(\\n        \\\"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\\\",\\n        header=None,\\n        names=cols_audiology,\\n    )\\n    # index mapping\\n    # length = len(df_audiology.columns)\\n    # X = df_audiology.loc[:, : length - 3].values\\n    # y = df_audiology.loc[:, length - 1].values\\n    df_audiology = df_audiology.drop(\\\"p_index\\\", axis=1)\\n    labels_col = df_audiology.pop(\\\"labels\\\")\\n    df_audiology.insert(0, \\\"labels\\\", labels_col)\\n    return df_audiology\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utility functions\n",
    "\n",
    "cols_mushroom = [\n",
    "    \"labels\",\n",
    "    \"cap_shape\",\n",
    "    \"cap_surface\",\n",
    "    \"cap_color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill_attachment\",\n",
    "    \"gill_spacing\",\n",
    "    \"gill_size\",\n",
    "    \"gill_color\",\n",
    "    \"stalk_shape\",\n",
    "    \"stalk_root\",\n",
    "    \"stalk_surface_above_ring\",\n",
    "    \"stalk_surface_below_ring\",\n",
    "    \"stalk_color_above_ring\",\n",
    "    \"stalk_color_below_ring\",\n",
    "    \"veil_type\",\n",
    "    \"veil_color\",\n",
    "    \"ring_number\",\n",
    "    \"ring_type\",\n",
    "    \"spore_print_color\",\n",
    "    \"population\",\n",
    "    \"habitat\",\n",
    "]\n",
    "\n",
    "cols_car = [\"buying\", \"maintenance\", \"doors\", \"passengers\", \"boot\", \"safety\", \"labels\"]\n",
    "\n",
    "cols_audiology = [\n",
    "    \"age_gt_60\",\n",
    "    \"air\",\n",
    "    \"airBoneGap\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bone\",\n",
    "    \"boneAbnormal\",\n",
    "    \"bser\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"speech\",\n",
    "    \"static_normal\",\n",
    "    \"tymp\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "    \"p_index\",\n",
    "    \"labels\",\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "0-5 -> data\n",
    "6 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_car():\n",
    "    df_car = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "        header=None,\n",
    "        names=cols_car,\n",
    "    )\n",
    "    # mappings using indexes:\n",
    "    # X = df_car.loc[:, :5].values\n",
    "    # y = df_car.loc[:, 6].values\n",
    "    labels_col = df_car.pop(\"labels\")\n",
    "    df_car.insert(0, \"labels\", labels_col)\n",
    "    return df_car\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "1-22 -> data\n",
    "0 -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_mushroom():\n",
    "    df_mushroom = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "        header=None,\n",
    "        names=cols_mushroom,\n",
    "    )\n",
    "    # index mappings\n",
    "    # X = df_mushroom.loc[:, 1:].values\n",
    "    # y = df_mushroom.loc[:, 0].values\n",
    "    # drop values corelating a bit too much like this\n",
    "    df_mushroom = df_mushroom.drop(\"odor\", axis=1)\n",
    "    df_mushroom = df_mushroom.drop(\"spore_print_color\", axis=1)\n",
    "    return df_mushroom\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29\n",
    "0:length-2 -> data\n",
    "length-1 unique id (p1-p200)\n",
    "length -> labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_audiology():\n",
    "    df_audiology = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/audiology/audiology.standardized.data\",\n",
    "        header=None,\n",
    "        names=cols_audiology,\n",
    "    )\n",
    "    # index mapping\n",
    "    # length = len(df_audiology.columns)\n",
    "    # X = df_audiology.loc[:, : length - 3].values\n",
    "    # y = df_audiology.loc[:, length - 1].values\n",
    "    df_audiology = df_audiology.drop(\"p_index\", axis=1)\n",
    "    labels_col = df_audiology.pop(\"labels\")\n",
    "    df_audiology.insert(0, \"labels\", labels_col)\n",
    "    return df_audiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3ea44b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   labels                    8124 non-null   object\n",
      " 1   cap_shape                 8124 non-null   object\n",
      " 2   cap_surface               8124 non-null   object\n",
      " 3   cap_color                 8124 non-null   object\n",
      " 4   bruises                   8124 non-null   object\n",
      " 5   gill_attachment           8124 non-null   object\n",
      " 6   gill_spacing              8124 non-null   object\n",
      " 7   gill_size                 8124 non-null   object\n",
      " 8   gill_color                8124 non-null   object\n",
      " 9   stalk_shape               8124 non-null   object\n",
      " 10  stalk_root                8124 non-null   object\n",
      " 11  stalk_surface_above_ring  8124 non-null   object\n",
      " 12  stalk_surface_below_ring  8124 non-null   object\n",
      " 13  stalk_color_above_ring    8124 non-null   object\n",
      " 14  stalk_color_below_ring    8124 non-null   object\n",
      " 15  veil_type                 8124 non-null   object\n",
      " 16  veil_color                8124 non-null   object\n",
      " 17  ring_number               8124 non-null   object\n",
      " 18  ring_type                 8124 non-null   object\n",
      " 19  population                8124 non-null   object\n",
      " 20  habitat                   8124 non-null   object\n",
      "dtypes: object(21)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "Size of X:  (8124, 20)\n",
      "Size of y:  (8124,)\n",
      "Size of dataset costs:  (1, 20)\n",
      "Cost of classification on full dataset:  124567\n",
      "Data has been split.\n",
      "Labels encoded:  (6499,) ,  (1625,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_formatted_code = \"# Choose dataset\\n# dataset = load_car()\\ndataset = load_mushroom()\\n# dataset = load_audiology()\\n\\nprint(dataset.info())\\n# print(\\\"First five records:\\\")\\n# print(dataset.head())\\n\\n# Extract to X and y\\nX_cat = dataset.loc[:, dataset.columns != \\\"labels\\\"]\\ny_cat = dataset.loc[:, \\\"labels\\\"]\\n\\nprint(\\\"Size of X: \\\", np.shape(X_cat))\\nprint(\\\"Size of y: \\\", np.shape(y_cat))\\n\\n# Generate a matrix of costs\\nmax_cost_allowed = 10000\\n\\ndataset_costs = pd.DataFrame(\\n    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\\n    columns=X_cat.columns,\\n)\\n\\nprint(\\\"Size of dataset costs: \\\", np.shape(dataset_costs))\\nprint(\\\"Cost of classification on full dataset: \\\", dataset_costs.sum(axis=1)[0])\\n\\nmax_seed_val = 2 ** 32 - 1\\n\\n# Split dataset into training set and test set\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\\n)\\nprint(\\\"Data has been split.\\\")\\n# print(\\\"X contains features: \\\", X_train.columns == \\\"index\\\")\\n\\n# Transform y using label encoder\\nle = LabelEncoder().fit(y_cat)\\nencoded_y_train = le.transform(y_train)\\nencoded_y_test = le.transform(y_test)\\nprint(\\\"Labels encoded: \\\", np.shape(encoded_y_train), \\\", \\\", np.shape(encoded_y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose dataset\n",
    "# dataset = load_car()\n",
    "dataset = load_mushroom()\n",
    "# dataset = load_audiology()\n",
    "\n",
    "print(dataset.info())\n",
    "# print(\"First five records:\")\n",
    "# print(dataset.head())\n",
    "\n",
    "# Extract to X and y\n",
    "X_cat = dataset.loc[:, dataset.columns != \"labels\"]\n",
    "y_cat = dataset.loc[:, \"labels\"]\n",
    "\n",
    "print(\"Size of X: \", np.shape(X_cat))\n",
    "print(\"Size of y: \", np.shape(y_cat))\n",
    "\n",
    "# Generate a matrix of costs\n",
    "max_cost_allowed = 10000\n",
    "\n",
    "dataset_costs = pd.DataFrame(\n",
    "    np.random.randint(0, max_cost_allowed, size=(1, np.shape(X_cat)[1])),\n",
    "    columns=X_cat.columns,\n",
    ")\n",
    "\n",
    "print(\"Size of dataset costs: \", np.shape(dataset_costs))\n",
    "print(\"Cost of classification on full dataset: \", dataset_costs.sum(axis=1)[0])\n",
    "\n",
    "max_seed_val = 2 ** 32 - 1\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=random.randrange(0, max_seed_val),\n",
    ")\n",
    "print(\"Data has been split.\")\n",
    "# print(\"X contains features: \", X_train.columns == \"index\")\n",
    "\n",
    "# Transform y using label encoder\n",
    "le = LabelEncoder().fit(y_cat)\n",
    "encoded_y_train = le.transform(y_train)\n",
    "encoded_y_test = le.transform(y_test)\n",
    "print(\"Labels encoded: \", np.shape(encoded_y_train), \", \", np.shape(encoded_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba83093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols created.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap_shape\\\",\\n    \\\"cap_surface\\\",\\n    \\\"cap_color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill_attachment\\\",\\n    \\\"gill_spacing\\\",\\n    \\\"gill_size\\\",\\n    \\\"gill_color\\\",\\n    \\\"stalk_shape\\\",\\n    \\\"stalk_root\\\",\\n    \\\"stalk_surface_above_ring\\\",\\n    \\\"stalk_surface_below_ring\\\",\\n    \\\"stalk_color_above_ring\\\",\\n    \\\"stalk_color_below_ring\\\",\\n    \\\"veil_type\\\",\\n    \\\"veil_color\\\",\\n    \\\"ring_number\\\",\\n    \\\"ring_type\\\",\\n    \\\"spore_print_color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_formatted_code = \"# Collectors of values\\n\\ncols_one_hot = [\\n    \\\"cap_shape\\\",\\n    \\\"cap_surface\\\",\\n    \\\"cap_color\\\",\\n    \\\"bruises\\\",\\n    \\\"odor\\\",\\n    \\\"gill_attachment\\\",\\n    \\\"gill_spacing\\\",\\n    \\\"gill_size\\\",\\n    \\\"gill_color\\\",\\n    \\\"stalk_shape\\\",\\n    \\\"stalk_root\\\",\\n    \\\"stalk_surface_above_ring\\\",\\n    \\\"stalk_surface_below_ring\\\",\\n    \\\"stalk_color_above_ring\\\",\\n    \\\"stalk_color_below_ring\\\",\\n    \\\"veil_type\\\",\\n    \\\"veil_color\\\",\\n    \\\"ring_number\\\",\\n    \\\"ring_type\\\",\\n    \\\"spore_print_color\\\",\\n    \\\"habitat\\\",\\n    \\\"age_gt_60\\\",\\n    \\\"airBoneGap\\\",\\n    \\\"boneAbnormal\\\",\\n    \\\"history_buzzing\\\",\\n    \\\"history_dizziness\\\",\\n    \\\"history_fluctuating\\\",\\n    \\\"history_fullness\\\",\\n    \\\"history_heredity\\\",\\n    \\\"history_nausea\\\",\\n    \\\"history_noise\\\",\\n    \\\"history_recruitment\\\",\\n    \\\"history_ringing\\\",\\n    \\\"history_roaring\\\",\\n    \\\"history_vomiting\\\",\\n    \\\"late_wave_poor\\\",\\n    \\\"m_at_2k\\\",\\n    \\\"m_cond_lt_1k\\\",\\n    \\\"m_gt_1k\\\",\\n    \\\"m_m_gt_2k\\\",\\n    \\\"m_m_sn\\\",\\n    \\\"m_m_sn_gt_1k\\\",\\n    \\\"m_m_sn_gt_2k\\\",\\n    \\\"m_m_sn_gt_500\\\",\\n    \\\"m_p_sn_gt_2k\\\",\\n    \\\"m_s_gt_500\\\",\\n    \\\"m_s_sn\\\",\\n    \\\"m_s_sn_gt_1k\\\",\\n    \\\"m_s_sn_gt_2k\\\",\\n    \\\"m_s_sn_gt_3k\\\",\\n    \\\"m_s_sn_gt_4k\\\",\\n    \\\"m_sn_2_3k\\\",\\n    \\\"m_sn_gt_1k\\\",\\n    \\\"m_sn_gt_2k\\\",\\n    \\\"m_sn_gt_3k\\\",\\n    \\\"m_sn_gt_4k\\\",\\n    \\\"m_sn_gt_500\\\",\\n    \\\"m_sn_gt_6k\\\",\\n    \\\"m_sn_lt_1k\\\",\\n    \\\"m_sn_lt_2k\\\",\\n    \\\"m_sn_lt_3k\\\",\\n    \\\"middle_wave_poor\\\",\\n    \\\"mod_gt_4k\\\",\\n    \\\"mod_mixed\\\",\\n    \\\"vmod_s_mixed\\\",\\n    \\\"mod_s_sn_gt_500\\\",\\n    \\\"mod_sn\\\",\\n    \\\"mod_sn_gt_1k\\\",\\n    \\\"mod_sn_gt_2k\\\",\\n    \\\"mod_sn_gt_3k\\\",\\n    \\\"mod_sn_gt_4k\\\",\\n    \\\"mod_sn_gt_500\\\",\\n    \\\"notch_4k\\\",\\n    \\\"notch_at_4k\\\",\\n    \\\"s_sn_gt_1k\\\",\\n    \\\"s_sn_gt_2k\\\",\\n    \\\"s_sn_gt_4k\\\",\\n    \\\"static_normal\\\",\\n    \\\"viith_nerve_signs\\\",\\n    \\\"wave_V_delayed\\\",\\n    \\\"waveform_ItoV_prolonged\\\",\\n]\\n\\ncols_ordinal = [\\n    \\\"buying\\\",\\n    \\\"maintenance\\\",\\n    \\\"doors\\\",\\n    \\\"passengers\\\",\\n    \\\"boot\\\",\\n    \\\"safety\\\",\\n    \\\"population\\\",\\n    \\\"air\\\",\\n    \\\"ar_c\\\",\\n    \\\"ar_u\\\",\\n    \\\"bser\\\",\\n    \\\"bone\\\",\\n    \\\"o_ar_c\\\",\\n    \\\"o_ar_u\\\",\\n    \\\"speech\\\",\\n    \\\"tymp\\\",\\n]\\n\\nprint(\\\"Cols created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collectors of values\n",
    "\n",
    "cols_one_hot = [\n",
    "    \"cap_shape\",\n",
    "    \"cap_surface\",\n",
    "    \"cap_color\",\n",
    "    \"bruises\",\n",
    "    \"odor\",\n",
    "    \"gill_attachment\",\n",
    "    \"gill_spacing\",\n",
    "    \"gill_size\",\n",
    "    \"gill_color\",\n",
    "    \"stalk_shape\",\n",
    "    \"stalk_root\",\n",
    "    \"stalk_surface_above_ring\",\n",
    "    \"stalk_surface_below_ring\",\n",
    "    \"stalk_color_above_ring\",\n",
    "    \"stalk_color_below_ring\",\n",
    "    \"veil_type\",\n",
    "    \"veil_color\",\n",
    "    \"ring_number\",\n",
    "    \"ring_type\",\n",
    "    \"spore_print_color\",\n",
    "    \"habitat\",\n",
    "    \"age_gt_60\",\n",
    "    \"airBoneGap\",\n",
    "    \"boneAbnormal\",\n",
    "    \"history_buzzing\",\n",
    "    \"history_dizziness\",\n",
    "    \"history_fluctuating\",\n",
    "    \"history_fullness\",\n",
    "    \"history_heredity\",\n",
    "    \"history_nausea\",\n",
    "    \"history_noise\",\n",
    "    \"history_recruitment\",\n",
    "    \"history_ringing\",\n",
    "    \"history_roaring\",\n",
    "    \"history_vomiting\",\n",
    "    \"late_wave_poor\",\n",
    "    \"m_at_2k\",\n",
    "    \"m_cond_lt_1k\",\n",
    "    \"m_gt_1k\",\n",
    "    \"m_m_gt_2k\",\n",
    "    \"m_m_sn\",\n",
    "    \"m_m_sn_gt_1k\",\n",
    "    \"m_m_sn_gt_2k\",\n",
    "    \"m_m_sn_gt_500\",\n",
    "    \"m_p_sn_gt_2k\",\n",
    "    \"m_s_gt_500\",\n",
    "    \"m_s_sn\",\n",
    "    \"m_s_sn_gt_1k\",\n",
    "    \"m_s_sn_gt_2k\",\n",
    "    \"m_s_sn_gt_3k\",\n",
    "    \"m_s_sn_gt_4k\",\n",
    "    \"m_sn_2_3k\",\n",
    "    \"m_sn_gt_1k\",\n",
    "    \"m_sn_gt_2k\",\n",
    "    \"m_sn_gt_3k\",\n",
    "    \"m_sn_gt_4k\",\n",
    "    \"m_sn_gt_500\",\n",
    "    \"m_sn_gt_6k\",\n",
    "    \"m_sn_lt_1k\",\n",
    "    \"m_sn_lt_2k\",\n",
    "    \"m_sn_lt_3k\",\n",
    "    \"middle_wave_poor\",\n",
    "    \"mod_gt_4k\",\n",
    "    \"mod_mixed\",\n",
    "    \"vmod_s_mixed\",\n",
    "    \"mod_s_sn_gt_500\",\n",
    "    \"mod_sn\",\n",
    "    \"mod_sn_gt_1k\",\n",
    "    \"mod_sn_gt_2k\",\n",
    "    \"mod_sn_gt_3k\",\n",
    "    \"mod_sn_gt_4k\",\n",
    "    \"mod_sn_gt_500\",\n",
    "    \"notch_4k\",\n",
    "    \"notch_at_4k\",\n",
    "    \"s_sn_gt_1k\",\n",
    "    \"s_sn_gt_2k\",\n",
    "    \"s_sn_gt_4k\",\n",
    "    \"static_normal\",\n",
    "    \"viith_nerve_signs\",\n",
    "    \"wave_V_delayed\",\n",
    "    \"waveform_ItoV_prolonged\",\n",
    "]\n",
    "\n",
    "cols_ordinal = [\n",
    "    \"buying\",\n",
    "    \"maintenance\",\n",
    "    \"doors\",\n",
    "    \"passengers\",\n",
    "    \"boot\",\n",
    "    \"safety\",\n",
    "    \"population\",\n",
    "    \"air\",\n",
    "    \"ar_c\",\n",
    "    \"ar_u\",\n",
    "    \"bser\",\n",
    "    \"bone\",\n",
    "    \"o_ar_c\",\n",
    "    \"o_ar_u\",\n",
    "    \"speech\",\n",
    "    \"tymp\",\n",
    "]\n",
    "\n",
    "print(\"Cols created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6bfe05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order created.\n",
      "    buying maintenance    doors passengers     boot   safety population  \\\n",
      "0      low         low        2          2    small      low          y   \n",
      "1      med         med        3          4      med      med          v   \n",
      "2     high        high        4       more      big     high          s   \n",
      "3    vhigh       vhigh    5more    filler1  filler1  filler1          n   \n",
      "4  filler1     filler1  filler1    filler2  filler2  filler2          c   \n",
      "5  filler2     filler2  filler2    filler3  filler3  filler3          a   \n",
      "6  filler3     filler3  filler3    filler4  filler4  filler4    filler1   \n",
      "\n",
      "        air      ar_c      ar_u      bser        bone    o_ar_c    o_ar_u  \\\n",
      "0    normal         ?         ?         ?           ?         ?         ?   \n",
      "1      mild    absent    absent    normal  unmeasured    absent    absent   \n",
      "2  moderate    normal    normal  degraded      normal    normal    normal   \n",
      "3    severe  elevated  elevated   filler1        mild  elevated  elevated   \n",
      "4  profound   filler1   filler1   filler2    moderate   filler1   filler1   \n",
      "5   filler1   filler2   filler2   filler3     filler1   filler2   filler2   \n",
      "6   filler2   filler3   filler3   filler4     filler3   filler3   filler3   \n",
      "\n",
      "       speech     tymp  \n",
      "0           ?        a  \n",
      "1  unmeasured       as  \n",
      "2   very_poor        b  \n",
      "3        poor       ad  \n",
      "4      normal        c  \n",
      "5        good  filler1  \n",
      "6   very_good  filler2  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_formatted_code = \"# Make order of categories per each column in ordinal_columns\\norder_of_ordinal_categories = pd.DataFrame.from_dict(\\n    {\\n        \\\"buying\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"maintenance\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"vhigh\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"doors\\\": [\\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"passengers\\\": [\\\"2\\\", \\\"4\\\", \\\"more\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"boot\\\": [\\\"small\\\", \\\"med\\\", \\\"big\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"safety\\\": [\\\"low\\\", \\\"med\\\", \\\"high\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"population\\\": [\\\"y\\\", \\\"v\\\", \\\"s\\\", \\\"n\\\", \\\"c\\\", \\\"a\\\", \\\"filler1\\\"],\\n        \\\"air\\\": [\\n            \\\"normal\\\",\\n            \\\"mild\\\",\\n            \\\"moderate\\\",\\n            \\\"severe\\\",\\n            \\\"profound\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n        ],\\n        \\\"ar_c\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"ar_u\\\": [\\\"?\\\", \\\"absent\\\", \\\"normal\\\", \\\"elevated\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\"],\\n        \\\"bser\\\": [\\\"?\\\", \\\"normal\\\", \\\"degraded\\\", \\\"filler1\\\", \\\"filler2\\\", \\\"filler3\\\", \\\"filler4\\\"],\\n        \\\"bone\\\": [\\\"?\\\", \\\"unmeasured\\\", \\\"normal\\\", \\\"mild\\\", \\\"moderate\\\", \\\"filler1\\\", \\\"filler3\\\"],\\n        \\\"o_ar_c\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"o_ar_u\\\": [\\n            \\\"?\\\",\\n            \\\"absent\\\",\\n            \\\"normal\\\",\\n            \\\"elevated\\\",\\n            \\\"filler1\\\",\\n            \\\"filler2\\\",\\n            \\\"filler3\\\",\\n        ],\\n        \\\"speech\\\": [\\n            \\\"?\\\",\\n            \\\"unmeasured\\\",\\n            \\\"very_poor\\\",\\n            \\\"poor\\\",\\n            \\\"normal\\\",\\n            \\\"good\\\",\\n            \\\"very_good\\\",\\n        ],\\n        \\\"tymp\\\": [\\\"a\\\", \\\"as\\\", \\\"b\\\", \\\"ad\\\", \\\"c\\\", \\\"filler1\\\", \\\"filler2\\\"],\\n    }\\n)\\n\\nprint(\\\"Order created.\\\")\\nprint(order_of_ordinal_categories)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make order of categories per each column in ordinal_columns\n",
    "order_of_ordinal_categories = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"buying\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"maintenance\": [\"low\", \"med\", \"high\", \"vhigh\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"doors\": [\"2\", \"3\", \"4\", \"5more\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"passengers\": [\"2\", \"4\", \"more\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"boot\": [\"small\", \"med\", \"big\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"safety\": [\"low\", \"med\", \"high\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"population\": [\"y\", \"v\", \"s\", \"n\", \"c\", \"a\", \"filler1\"],\n",
    "        \"air\": [\n",
    "            \"normal\",\n",
    "            \"mild\",\n",
    "            \"moderate\",\n",
    "            \"severe\",\n",
    "            \"profound\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "        ],\n",
    "        \"ar_c\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"ar_u\": [\"?\", \"absent\", \"normal\", \"elevated\", \"filler1\", \"filler2\", \"filler3\"],\n",
    "        \"bser\": [\"?\", \"normal\", \"degraded\", \"filler1\", \"filler2\", \"filler3\", \"filler4\"],\n",
    "        \"bone\": [\"?\", \"unmeasured\", \"normal\", \"mild\", \"moderate\", \"filler1\", \"filler3\"],\n",
    "        \"o_ar_c\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"o_ar_u\": [\n",
    "            \"?\",\n",
    "            \"absent\",\n",
    "            \"normal\",\n",
    "            \"elevated\",\n",
    "            \"filler1\",\n",
    "            \"filler2\",\n",
    "            \"filler3\",\n",
    "        ],\n",
    "        \"speech\": [\n",
    "            \"?\",\n",
    "            \"unmeasured\",\n",
    "            \"very_poor\",\n",
    "            \"poor\",\n",
    "            \"normal\",\n",
    "            \"good\",\n",
    "            \"very_good\",\n",
    "        ],\n",
    "        \"tymp\": [\"a\", \"as\", \"b\", \"ad\", \"c\", \"filler1\", \"filler2\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Order created.\")\n",
    "print(order_of_ordinal_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3c4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class EncodingCategoricalBayes has been created\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_formatted_code = \"# Create custom encoding categorical bayes classifier\\nclass EncodingCategoricalBayes:\\n    def __init__(\\n        self,\\n        # classifier,\\n        ordinal_categories_order,\\n        ordinal_columns,\\n        one_hot_columns,\\n        dataset,\\n    ):\\n        # self.classifier = classifier\\n        self.ordinal_categories_order = ordinal_categories_order\\n        self.ordinal_columns = ordinal_columns\\n        self.one_hot_columns = one_hot_columns\\n        self.transformer_dataset = dataset\\n\\n    def fit(self, X, y):\\n        self.classifier = CategoricalNB(min_categories=X.shape[0])\\n        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\\n        self.column_transformer.fit(X)\\n        return self.classifier.fit(self.encode_features(X), y)\\n        # return self.classifier.fit(X, y)\\n\\n    def predict(self, X):\\n        return self.classifier.predict(self.encode_features(X))\\n        # return self.classifier.predict(X)\\n\\n    def predict_proba(self, X):\\n        return self.classifier.predict_proba(self.encode_features(X))\\n        # return self.classifier.predict_proba(X)\\n\\n    def encode_features(self, X):\\n        encoded_X = self.column_transformer.transform(X)\\n        if scipy.sparse.issparse(encoded_X):\\n            encoded_X = encoded_X.toarray()\\n        return encoded_X\\n\\n    def make_column_transformer(self, X):\\n        # Get current ordinal and one hot columns\\n        total_column_list = X.select_dtypes(include=\\\"object\\\").columns\\n        # print(\\\"Total col list: \\\", total_column_list)\\n        current_columns_one_hot = self.collect_current_one_hot_columns(\\n            total_column_list\\n        )\\n        current_columns_ordinal = self.collect_current_ordinal_columns(\\n            total_column_list\\n        )\\n\\n        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\\n            current_columns_ordinal\\n        )\\n        \\\"\\\"\\\"\\n        print(\\n            \\\"Columns in column transformer: \\\",\\n            current_columns_one_hot,\\n            \\\" and \\\",\\n            current_columns_ordinal,\\n        )\\n        \\\"\\\"\\\"\\n\\n        # Create column transformer\\n        column_transformer = make_column_transformer(\\n            (OneHotEncoder(), current_columns_one_hot),\\n            (\\n                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\\n                current_columns_ordinal,\\n            ),\\n        )\\n        return column_transformer\\n\\n    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\\n        # Get common cols to feed them in proper order to ordinal encoder\\n        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\\n            argColumns\\n        )\\n        # Convert to list\\n        order_of_ordinal_categories_list = (\\n            self.ordinal_categories_order[index_of_common_cols]\\n            .values.transpose()\\n            .tolist()\\n        )\\n        return order_of_ordinal_categories_list\\n\\n    def intersection(self, lst1, lst2):\\n        # collects common elements in both lists\\n        return [value for value in lst1 if value in lst2]\\n\\n    def collect_current_one_hot_columns(self, argCols):\\n        return self.intersection(self.one_hot_columns, argCols)\\n\\n    def collect_current_ordinal_columns(self, argCols):\\n        # make list of all values and create steps for them\\n        return self.intersection(self.ordinal_columns, argCols)\\n\\n    def get_params(self, deep=True):\\n        return self.classifier.get_params()\\n\\n\\nprint(\\\"Class EncodingCategoricalBayes has been created\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create custom encoding categorical bayes classifier\n",
    "class EncodingCategoricalBayes:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # classifier,\n",
    "        ordinal_categories_order,\n",
    "        ordinal_columns,\n",
    "        one_hot_columns,\n",
    "        dataset,\n",
    "    ):\n",
    "        # self.classifier = classifier\n",
    "        self.ordinal_categories_order = ordinal_categories_order\n",
    "        self.ordinal_columns = ordinal_columns\n",
    "        self.one_hot_columns = one_hot_columns\n",
    "        self.transformer_dataset = dataset\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier = CategoricalNB(min_categories=X.shape[0])\n",
    "        self.column_transformer = self.make_column_transformer(self.transformer_dataset)\n",
    "        self.column_transformer.fit(X)\n",
    "        return self.classifier.fit(self.encode_features(X), y)\n",
    "        # return self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(self.encode_features(X))\n",
    "        # return self.classifier.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.classifier.predict_proba(self.encode_features(X))\n",
    "        # return self.classifier.predict_proba(X)\n",
    "\n",
    "    def encode_features(self, X):\n",
    "        encoded_X = self.column_transformer.transform(X)\n",
    "        if scipy.sparse.issparse(encoded_X):\n",
    "            encoded_X = encoded_X.toarray()\n",
    "        return encoded_X\n",
    "\n",
    "    def make_column_transformer(self, X):\n",
    "        # Get current ordinal and one hot columns\n",
    "        total_column_list = X.select_dtypes(include=\"object\").columns\n",
    "        # print(\"Total col list: \", total_column_list)\n",
    "        current_columns_one_hot = self.collect_current_one_hot_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "        current_columns_ordinal = self.collect_current_ordinal_columns(\n",
    "            total_column_list\n",
    "        )\n",
    "\n",
    "        current_ordinal_col_ordering_to_encode = self.calculate_current_order_of_ordinal_columns_to_encode(\n",
    "            current_columns_ordinal\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"Columns in column transformer: \",\n",
    "            current_columns_one_hot,\n",
    "            \" and \",\n",
    "            current_columns_ordinal,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Create column transformer\n",
    "        column_transformer = make_column_transformer(\n",
    "            (OneHotEncoder(), current_columns_one_hot),\n",
    "            (\n",
    "                OrdinalEncoder(categories=current_ordinal_col_ordering_to_encode),\n",
    "                current_columns_ordinal,\n",
    "            ),\n",
    "        )\n",
    "        return column_transformer\n",
    "\n",
    "    def calculate_current_order_of_ordinal_columns_to_encode(self, argColumns):\n",
    "        # Get common cols to feed them in proper order to ordinal encoder\n",
    "        index_of_common_cols = self.ordinal_categories_order.columns.intersection(\n",
    "            argColumns\n",
    "        )\n",
    "        # Convert to list\n",
    "        order_of_ordinal_categories_list = (\n",
    "            self.ordinal_categories_order[index_of_common_cols]\n",
    "            .values.transpose()\n",
    "            .tolist()\n",
    "        )\n",
    "        return order_of_ordinal_categories_list\n",
    "\n",
    "    def intersection(self, lst1, lst2):\n",
    "        # collects common elements in both lists\n",
    "        return [value for value in lst1 if value in lst2]\n",
    "\n",
    "    def collect_current_one_hot_columns(self, argCols):\n",
    "        return self.intersection(self.one_hot_columns, argCols)\n",
    "\n",
    "    def collect_current_ordinal_columns(self, argCols):\n",
    "        # make list of all values and create steps for them\n",
    "        return self.intersection(self.ordinal_columns, argCols)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.classifier.get_params()\n",
    "\n",
    "\n",
    "print(\"Class EncodingCategoricalBayes has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1b940f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Forward Feature Selector is created.\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n        data_duplication_flag,\\n    ):\\n        # Make copies as we'll be altering these datasets\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        # Variables\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n        duplicates = pd.DataFrame()\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # Statistics\\n        loop_number = 0\\n        total_number_of_cases = np.shape(X_tst)[0]\\n        classified_size = 0\\n\\n        # Main loop, until all test classes are classified\\n        while unused_features and not X_tst.empty:\\n            # make a list of features with their predicted accuracy\\n            print(\\\"Start of loop number: \\\", loop_number)\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            print(\\\"Picked feature: \\\", best_next_feature)\\n            print(\\\"Current feature set: \\\", current_features)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas, duplicates = self.predict_proba_wrapper(\\n                classifier,\\n                X_train_subset_full,\\n                y_train,\\n                X_test_subset_full,\\n                duplicates,\\n                data_duplication_flag,\\n                best_next_feature,\\n            )\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost_of_classification\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            current_features_string = \\\",\\\".join(map(str, current_features))\\n\\n            feature_series = current_features_string * np.shape(X_tst)[0]\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"feature_used_to_classify\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_probas[\\\"highest_proba\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n\\n            batch_result_dataframe = pd.concat(\\n                [\\n                    outcomes,\\n                    highest_probas,\\n                    cost_of_classification,\\n                    feature_used_to_classify,\\n                ],\\n                axis=1,\\n            )\\n\\n            # print(\\\"Final result dataframe:\\\")\\n            # print(batch_result_dataframe)\\n            # print(\\\"Are X test and result dataframe indexes equal? :\\\",X_tst.index.equals(batch_result_dataframe.index))\\n\\n            rows_classified = highest_probas.loc[condition].index\\n            rows_unclassified = X_tst.index.difference(rows_classified)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n\\n            # print(\\\"Rows classified: \\\", rows_classified)\\n            # print(\\\"Rows unclassified: \\\", rows_unclassified)\\n            # print(\\\"X test before:\\\")\\n            # print(X_tst)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows_classified)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(rows_unclassified)[0])\\n\\n            X_tst.drop(rows_classified, inplace=True)\\n            y_tst.drop(rows_classified, inplace=True)\\n            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\\n            if unused_features:\\n                batch_result_dataframe.drop(rows_unclassified, inplace=True)\\n            # print(\\\"Are X test and result dataframe indexes unequal? :\\\",not X_tst.index.equals(batch_result_dataframe.index),)\\n\\n            # print(\\\"final: \\\")\\n            # print(batch_result_dataframe)\\n            final_result_dataframe = pd.concat(\\n                [final_result_dataframe, batch_result_dataframe]\\n            )\\n\\n            # percentage classified\\n            classified_size += rows_classified.size\\n            print(\\\"End of loop \\\", loop_number)\\n            loop_number += 1\\n            print(\\n                \\\"Classified classes: \\\",\\n                classified_size,\\n                \\\"/\\\",\\n                total_number_of_cases,\\n                \\\" | \\\",\\n                \\\"{:.2f}\\\".format(classified_size / total_number_of_cases * 100),\\n                \\\"%\\\",\\n            )\\n            # now go back to the beginning of the loop and check for unclassified classes\\n\\n        print(\\\"Out of the loop. Usable features ran out, or no more cases to classify.\\\")\\n        return final_result_dataframe\\n\\n    def predict_proba_wrapper(\\n        self, classifier, X_train, y_train, X_test, duplicates, flag, unknown_feature\\n    ):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"outcome\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_proba\\\"])\\n        to_duplicate_next = pd.DataFrame()\\n        for index, row_entry in X_test.iterrows():\\n            row_features = row_entry.axes[0].tolist()\\n            row_features.remove(unknown_feature)\\n            feature_data = pd.DataFrame()\\n            if row_features is not None:\\n                feature_data = row_entry.loc[row_features]\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                X_train, y_train, feature_data, flag,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"outcome\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_proba\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn, to_duplicate_next\\n\\n    def prepare_train_dataset(\\n        self, X_train_arg, y_train_arg, duplicates_per_case, flag\\n    ):\\n        if duplicates_per_case.empty or not flag:\\n            # nothing to dupe or flag is down (skip)\\n            return X_train_arg, y_train_arg\\n\\n        X_train = copy.deepcopy(X_train_arg)\\n        y_train = copy.deepcopy(y_train_arg)\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = pd.concat([X_train, y_train], axis=1)\\n\\n        # for each feature\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates_per_case.axes[0].tolist():\\n            dupes = full_test_data.apply(\\n                lambda row: row[\\n                    full_test_data[col_name].isin([duplicates_per_case[col_name]])\\n                ]\\n            )\\n            duplicate_rows = pd.concat(\\n                [duplicate_rows, dupes], axis=0, ignore_index=True\\n            )\\n        full_test_data = pd.concat(\\n            [full_test_data, duplicate_rows], axis=0, ignore_index=True\\n        )\\n\\n        # print(\\\"Delta:\\\", np.shape(full_test_data)[0])\\n\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\\n\\nprint(type(encoded_y_test))\";\n",
       "                var nbb_formatted_code = \"# Sequential Forward Feature Selector\\nclass SequentialForwardFeatureSelector:\\n    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\\n        self.classification_costs = classification_costs\\n        self.CV_folds = CV_folds\\n        self.uncertainty_threshold = uncertainty_threshold\\n\\n    def sequential_predict(\\n        self,\\n        X_train_original,\\n        y_train_original,\\n        X_test_original,\\n        y_test_original,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n        whole_dataset,\\n        data_duplication_flag,\\n    ):\\n        # Make copies as we'll be altering these datasets\\n        X_tr = copy.deepcopy(X_train_original)\\n        y_tr = copy.deepcopy(y_train_original)\\n        X_tst = copy.deepcopy(X_test_original)\\n        y_tst = copy.deepcopy(y_test_original)\\n\\n        # Variables\\n        final_result_columns = [\\n            \\\"highest_proba\\\",\\n            \\\"outcome\\\",\\n            \\\"cost_of_classification\\\",\\n            \\\"feature_used_to_classify\\\",\\n        ]\\n        duplicates = pd.DataFrame()\\n        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\\n        unused_features = X_tst.columns.tolist()\\n        current_features = []\\n\\n        # Statistics\\n        loop_number = 0\\n        total_number_of_cases = np.shape(X_tst)[0]\\n        classified_size = 0\\n\\n        # Main loop, until all test classes are classified\\n        while unused_features and not X_tst.empty:\\n            # make a list of features with their predicted accuracy\\n            print(\\\"Start of loop number: \\\", loop_number)\\n            accuracy_per_new_feature = self.find_next_best_feature(\\n                X_tr,\\n                y_tr,\\n                X_test,\\n                y_test,\\n                unused_features,\\n                current_features,\\n                whole_dataset,\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n            )\\n\\n            # we have all features and their accuracies, we pick the best one\\n            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\\n            # and adjust feature trackers\\n            unused_features.remove(best_next_feature)\\n            current_features.append(best_next_feature)\\n\\n            print(\\\"Picked feature: \\\", best_next_feature)\\n            print(\\\"Current feature set: \\\", current_features)\\n\\n            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\\n\\n            # create X_train subset with apropriate features\\n            X_train_subset_full = pd.DataFrame(X_tr[current_features])\\n            X_test_subset_full = pd.DataFrame(X_tst[current_features])\\n            # make new classifier (due to different encoder data)\\n            classifier = self.make_encoding_categorical_bayes(\\n                ordinal_categoires_order,\\n                cols_ordinal,\\n                cols_one_hot,\\n                pd.DataFrame(dataset[current_features]),\\n            )\\n            # define duplicates before end of refactorization and moving forward\\n            outcomes, highest_probas, duplicates = self.predict_proba_wrapper(\\n                classifier,\\n                X_train_subset_full,\\n                y_train,\\n                X_test_subset_full,\\n                duplicates,\\n                data_duplication_flag,\\n                best_next_feature,\\n            )\\n\\n            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\\n\\n            cost_series = (\\n                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\\n            )  # remember to change to list of featureS\\n            cost_of_classification = pd.DataFrame(\\n                cost_series, columns=[\\\"cost_of_classification\\\"], index=X_tst.index\\n            )\\n            # print(cost_of_classification)\\n\\n            current_features_string = \\\",\\\".join(map(str, current_features))\\n\\n            feature_series = current_features_string * np.shape(X_tst)[0]\\n            feature_used_to_classify = pd.DataFrame(\\n                feature_series, columns=[\\\"feature_used_to_classify\\\"], index=X_tst.index\\n            )\\n            # print(feature_used_to_classify)\\n\\n            # remove already classified classes\\n            condition = highest_probas[\\\"highest_proba\\\"] > (\\n                1 - self.uncertainty_threshold\\n            )\\n\\n            batch_result_dataframe = pd.concat(\\n                [\\n                    outcomes,\\n                    highest_probas,\\n                    cost_of_classification,\\n                    feature_used_to_classify,\\n                ],\\n                axis=1,\\n            )\\n\\n            # print(\\\"Final result dataframe:\\\")\\n            # print(batch_result_dataframe)\\n            # print(\\\"Are X test and result dataframe indexes equal? :\\\",X_tst.index.equals(batch_result_dataframe.index))\\n\\n            rows_classified = highest_probas.loc[condition].index\\n            rows_unclassified = X_tst.index.difference(rows_classified)\\n            y_tst = pd.DataFrame(y_tst, columns=[\\\"labels\\\"], index=X_tst.index)\\n\\n            # print(\\\"Rows classified: \\\", rows_classified)\\n            # print(\\\"Rows unclassified: \\\", rows_unclassified)\\n            # print(\\\"X test before:\\\")\\n            # print(X_tst)\\n            # print(\\\"Number of rows to drop:\\\", np.shape(rows_classified)[0])\\n            # print(\\\"Number of rows to leave:\\\", np.shape(rows_unclassified)[0])\\n\\n            X_tst.drop(rows_classified, inplace=True)\\n            y_tst.drop(rows_classified, inplace=True)\\n            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\\n            if unused_features:\\n                batch_result_dataframe.drop(rows_unclassified, inplace=True)\\n            # print(\\\"Are X test and result dataframe indexes unequal? :\\\",not X_tst.index.equals(batch_result_dataframe.index),)\\n\\n            # print(\\\"final: \\\")\\n            # print(batch_result_dataframe)\\n            final_result_dataframe = pd.concat(\\n                [final_result_dataframe, batch_result_dataframe]\\n            )\\n\\n            # percentage classified\\n            classified_size += rows_classified.size\\n            print(\\\"End of loop \\\", loop_number)\\n            loop_number += 1\\n            print(\\n                \\\"Classified classes: \\\",\\n                classified_size,\\n                \\\"/\\\",\\n                total_number_of_cases,\\n                \\\" | \\\",\\n                \\\"{:.2f}\\\".format(classified_size / total_number_of_cases * 100),\\n                \\\"%\\\",\\n            )\\n            # now go back to the beginning of the loop and check for unclassified classes\\n\\n        print(\\\"Out of the loop. Usable features ran out, or no more cases to classify.\\\")\\n        return final_result_dataframe\\n\\n    def predict_proba_wrapper(\\n        self, classifier, X_train, y_train, X_test, duplicates, flag, unknown_feature\\n    ):\\n        outcomes_fn = pd.DataFrame(columns=[\\\"outcome\\\"])\\n        highest_probas_fn = pd.DataFrame(columns=[\\\"highest_proba\\\"])\\n        to_duplicate_next = pd.DataFrame()\\n        for index, row_entry in X_test.iterrows():\\n            row_features = row_entry.axes[0].tolist()\\n            row_features.remove(unknown_feature)\\n            feature_data = pd.DataFrame()\\n            if row_features is not None:\\n                feature_data = row_entry.loc[row_features]\\n            X_train_mod, y_train_mod = self.prepare_train_dataset(\\n                X_train, y_train, feature_data, flag,\\n            )\\n            classifier.fit(X_train_mod, y_train_mod)\\n\\n            df_row_entry = row_entry.to_frame().T\\n\\n            # gotta make it in 2 steps bc of no column name tracking in numpy\\n            new_outcome_df = pd.DataFrame(\\n                classifier.predict(df_row_entry),\\n                columns=[\\\"outcome\\\"],\\n                index=df_row_entry.index,\\n            )\\n\\n            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\\n\\n            probas_fn = classifier.predict_proba(df_row_entry)\\n            new_probas_df = pd.DataFrame(\\n                np.max(np.max(probas_fn, axis=1), axis=0),\\n                columns=[\\\"highest_proba\\\"],\\n                index=df_row_entry.index,\\n            )\\n            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\\n        return outcomes_fn, highest_probas_fn, to_duplicate_next\\n\\n    def prepare_train_dataset(\\n        self, X_train_arg, y_train_arg, duplicates_per_case, flag\\n    ):\\n        if duplicates_per_case.empty or not flag:\\n            # nothing to dupe or flag is down (skip)\\n            return X_train_arg, y_train_arg\\n\\n        X_train = copy.deepcopy(X_train_arg)\\n        y_train = copy.deepcopy(y_train_arg)\\n\\n        # make 1 full dataset for easy modification\\n        full_test_data = pd.concat([X_train, y_train], axis=1)\\n\\n        # for each feature\\n        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\\n        for col_name in duplicates_per_case.axes[0].tolist():\\n            dupes = full_test_data.apply(\\n                lambda row: row[\\n                    full_test_data[col_name].isin([duplicates_per_case[col_name]])\\n                ]\\n            )\\n            duplicate_rows = pd.concat(\\n                [duplicate_rows, dupes], axis=0, ignore_index=True\\n            )\\n        full_test_data = pd.concat(\\n            [full_test_data, duplicate_rows], axis=0, ignore_index=True\\n        )\\n\\n        # print(\\\"Delta:\\\", np.shape(full_test_data)[0])\\n\\n        # return X_train, y_train\\n        return (\\n            full_test_data.loc[:, full_test_data.columns != \\\"labels\\\"],\\n            full_test_data.loc[:, \\\"labels\\\"],\\n        )\\n\\n    def find_next_best_feature(\\n        self,\\n        X_tr,\\n        y_tr,\\n        X_test,\\n        y_test,\\n        unused_feat,\\n        current_feat,\\n        whole_dataset,\\n        ordinal_categoires_order,\\n        cols_ordinal,\\n        cols_one_hot,\\n    ):\\n        kf = KFold(n_splits=self.CV_folds)\\n        accuracy_per_new_feature = pd.DataFrame(\\n            0, index=np.arange(1), columns=unused_feat,\\n        )\\n        for new_feature in unused_feat:\\n            # print(\\\"Calculating feature: \\\", new_feature)\\n            sum_of_accuracies = 0\\n            feature_set_to_try = copy.deepcopy(current_feat)\\n            feature_set_to_try.append(new_feature)\\n            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\\n\\n            for train_index, test_index in kf.split(X_tr):\\n                # create _train, _cv_test, _test splits\\n                # no need to reshuffle it, it's already in random order\\n                # X is a dataframe\\n                X_train, X_cv = (\\n                    X_tr.iloc[train_index],\\n                    X_tr.iloc[test_index],\\n                )\\n\\n                # y is a numpy array\\n                y_train, y_cv = (\\n                    y_tr[train_index],\\n                    y_tr[test_index],\\n                )\\n\\n                # print(\\\"train: \\\", train_index, \\\"test: \\\", test_index)\\n\\n                # add feture to test to X\\n                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\\n                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\\n\\n                # make classifier\\n                classifier = self.make_encoding_categorical_bayes(\\n                    ordinal_categoires_order,\\n                    cols_ordinal,\\n                    cols_one_hot,\\n                    dataset_for_encoder,\\n                )\\n\\n                # train classifier\\n                classifier.fit(X_train_subset, y_train)\\n                # predict\\n                y_cv_prediciton = classifier.predict(X_cv_subset)\\n                # judge accuracy of new feature subset\\n                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\\n\\n            # save accuracy per new feature\\n            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\\n            # print(\\\"Finished calculations for feature: \\\",new_feature,\\\"Accuracy: \\\",accuracy_per_new_feature[new_feature][0])\\n\\n        return accuracy_per_new_feature\\n\\n    def make_encoding_categorical_bayes(\\n        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\\n    ):\\n        return EncodingCategoricalBayes(\\n            # classifier=CategoricalNB(),\\n            ordinal_categories_order=order_of_ordinal_categories,\\n            ordinal_columns=cols_ordinal,\\n            one_hot_columns=cols_one_hot,\\n            dataset=whole_dataset,\\n        )\\n\\n    def get_classification_costs(self, list_of_categories):\\n        return self.classification_costs[\\n            self.classification_costs.columns.intersection(list_of_categories)\\n        ].sum(axis=1)[0]\\n\\n\\nprint(\\\"Sequential Forward Feature Selector is created.\\\")\\n\\nprint(type(encoded_y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sequential Forward Feature Selector\n",
    "class SequentialForwardFeatureSelector:\n",
    "    def __init__(self, classification_costs, CV_folds, uncertainty_threshold):\n",
    "        self.classification_costs = classification_costs\n",
    "        self.CV_folds = CV_folds\n",
    "        self.uncertainty_threshold = uncertainty_threshold\n",
    "\n",
    "    def sequential_predict(\n",
    "        self,\n",
    "        X_train_original,\n",
    "        y_train_original,\n",
    "        X_test_original,\n",
    "        y_test_original,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "        whole_dataset,\n",
    "        data_duplication_flag,\n",
    "    ):\n",
    "        # Make copies as we'll be altering these datasets\n",
    "        X_tr = copy.deepcopy(X_train_original)\n",
    "        y_tr = copy.deepcopy(y_train_original)\n",
    "        X_tst = copy.deepcopy(X_test_original)\n",
    "        y_tst = copy.deepcopy(y_test_original)\n",
    "\n",
    "        # Variables\n",
    "        final_result_columns = [\n",
    "            \"highest_proba\",\n",
    "            \"outcome\",\n",
    "            \"cost_of_classification\",\n",
    "            \"feature_used_to_classify\",\n",
    "        ]\n",
    "        duplicates = pd.DataFrame()\n",
    "        final_result_dataframe = pd.DataFrame(columns=final_result_columns)\n",
    "        unused_features = X_tst.columns.tolist()\n",
    "        current_features = []\n",
    "\n",
    "        # Statistics\n",
    "        loop_number = 0\n",
    "        total_number_of_cases = np.shape(X_tst)[0]\n",
    "        classified_size = 0\n",
    "\n",
    "        # Main loop, until all test classes are classified\n",
    "        while unused_features and not X_tst.empty:\n",
    "            # make a list of features with their predicted accuracy\n",
    "            print(\"Start of loop number: \", loop_number)\n",
    "            accuracy_per_new_feature = self.find_next_best_feature(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                unused_features,\n",
    "                current_features,\n",
    "                whole_dataset,\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "            )\n",
    "\n",
    "            # we have all features and their accuracies, we pick the best one\n",
    "            best_next_feature = pd.DataFrame(accuracy_per_new_feature).idxmax(axis=1)[0]\n",
    "            # and adjust feature trackers\n",
    "            unused_features.remove(best_next_feature)\n",
    "            current_features.append(best_next_feature)\n",
    "\n",
    "            print(\"Picked feature: \", best_next_feature)\n",
    "            print(\"Current feature set: \", current_features)\n",
    "\n",
    "            # 1.We have chosen the best feature and we classify the test using the feature and checking the probablilities\n",
    "\n",
    "            # create X_train subset with apropriate features\n",
    "            X_train_subset_full = pd.DataFrame(X_tr[current_features])\n",
    "            X_test_subset_full = pd.DataFrame(X_tst[current_features])\n",
    "            # make new classifier (due to different encoder data)\n",
    "            classifier = self.make_encoding_categorical_bayes(\n",
    "                ordinal_categoires_order,\n",
    "                cols_ordinal,\n",
    "                cols_one_hot,\n",
    "                pd.DataFrame(dataset[current_features]),\n",
    "            )\n",
    "            # define duplicates before end of refactorization and moving forward\n",
    "            outcomes, highest_probas, duplicates = self.predict_proba_wrapper(\n",
    "                classifier,\n",
    "                X_train_subset_full,\n",
    "                y_train,\n",
    "                X_test_subset_full,\n",
    "                duplicates,\n",
    "                data_duplication_flag,\n",
    "                best_next_feature,\n",
    "            )\n",
    "\n",
    "            # 2.If proba > threshold, the we move/pop them from X_test to results along with the statistics\n",
    "\n",
    "            cost_series = (\n",
    "                [self.get_classification_costs(current_features)] * np.shape(X_tst)[0]\n",
    "            )  # remember to change to list of featureS\n",
    "            cost_of_classification = pd.DataFrame(\n",
    "                cost_series, columns=[\"cost_of_classification\"], index=X_tst.index\n",
    "            )\n",
    "            # print(cost_of_classification)\n",
    "\n",
    "            current_features_string = \",\".join(map(str, current_features))\n",
    "\n",
    "            feature_series = current_features_string * np.shape(X_tst)[0]\n",
    "            feature_used_to_classify = pd.DataFrame(\n",
    "                feature_series, columns=[\"feature_used_to_classify\"], index=X_tst.index\n",
    "            )\n",
    "            # print(feature_used_to_classify)\n",
    "\n",
    "            # remove already classified classes\n",
    "            condition = highest_probas[\"highest_proba\"] > (\n",
    "                1 - self.uncertainty_threshold\n",
    "            )\n",
    "\n",
    "            batch_result_dataframe = pd.concat(\n",
    "                [\n",
    "                    outcomes,\n",
    "                    highest_probas,\n",
    "                    cost_of_classification,\n",
    "                    feature_used_to_classify,\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            # print(\"Final result dataframe:\")\n",
    "            # print(batch_result_dataframe)\n",
    "            # print(\"Are X test and result dataframe indexes equal? :\",X_tst.index.equals(batch_result_dataframe.index))\n",
    "\n",
    "            rows_classified = highest_probas.loc[condition].index\n",
    "            rows_unclassified = X_tst.index.difference(rows_classified)\n",
    "            y_tst = pd.DataFrame(y_tst, columns=[\"labels\"], index=X_tst.index)\n",
    "\n",
    "            # print(\"Rows classified: \", rows_classified)\n",
    "            # print(\"Rows unclassified: \", rows_unclassified)\n",
    "            # print(\"X test before:\")\n",
    "            # print(X_tst)\n",
    "            # print(\"Number of rows to drop:\", np.shape(rows_classified)[0])\n",
    "            # print(\"Number of rows to leave:\", np.shape(rows_unclassified)[0])\n",
    "\n",
    "            X_tst.drop(rows_classified, inplace=True)\n",
    "            y_tst.drop(rows_classified, inplace=True)\n",
    "            # if unused features is empty, append everything -> if unused features is not empty, drop unneeded\n",
    "            if unused_features:\n",
    "                batch_result_dataframe.drop(rows_unclassified, inplace=True)\n",
    "            # print(\"Are X test and result dataframe indexes unequal? :\",not X_tst.index.equals(batch_result_dataframe.index),)\n",
    "\n",
    "            # print(\"final: \")\n",
    "            # print(batch_result_dataframe)\n",
    "            final_result_dataframe = pd.concat(\n",
    "                [final_result_dataframe, batch_result_dataframe]\n",
    "            )\n",
    "\n",
    "            # percentage classified\n",
    "            classified_size += rows_classified.size\n",
    "            print(\"End of loop \", loop_number)\n",
    "            loop_number += 1\n",
    "            print(\n",
    "                \"Classified classes: \",\n",
    "                classified_size,\n",
    "                \"/\",\n",
    "                total_number_of_cases,\n",
    "                \" | \",\n",
    "                \"{:.2f}\".format(classified_size / total_number_of_cases * 100),\n",
    "                \"%\",\n",
    "            )\n",
    "            # now go back to the beginning of the loop and check for unclassified classes\n",
    "\n",
    "        print(\"Out of the loop. Usable features ran out, or no more cases to classify.\")\n",
    "        return final_result_dataframe\n",
    "\n",
    "    def predict_proba_wrapper(\n",
    "        self, classifier, X_train, y_train, X_test, duplicates, flag, unknown_feature\n",
    "    ):\n",
    "        outcomes_fn = pd.DataFrame(columns=[\"outcome\"])\n",
    "        highest_probas_fn = pd.DataFrame(columns=[\"highest_proba\"])\n",
    "        to_duplicate_next = pd.DataFrame()\n",
    "        for index, row_entry in X_test.iterrows():\n",
    "            row_features = row_entry.axes[0].tolist()\n",
    "            row_features.remove(unknown_feature)\n",
    "            feature_data = pd.DataFrame()\n",
    "            if row_features is not None:\n",
    "                feature_data = row_entry.loc[row_features]\n",
    "            X_train_mod, y_train_mod = self.prepare_train_dataset(\n",
    "                X_train, y_train, feature_data, flag,\n",
    "            )\n",
    "            classifier.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "            df_row_entry = row_entry.to_frame().T\n",
    "\n",
    "            # gotta make it in 2 steps bc of no column name tracking in numpy\n",
    "            new_outcome_df = pd.DataFrame(\n",
    "                classifier.predict(df_row_entry),\n",
    "                columns=[\"outcome\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "\n",
    "            outcomes_fn = pd.concat([outcomes_fn, new_outcome_df])\n",
    "\n",
    "            probas_fn = classifier.predict_proba(df_row_entry)\n",
    "            new_probas_df = pd.DataFrame(\n",
    "                np.max(np.max(probas_fn, axis=1), axis=0),\n",
    "                columns=[\"highest_proba\"],\n",
    "                index=df_row_entry.index,\n",
    "            )\n",
    "            highest_probas_fn = pd.concat([highest_probas_fn, new_probas_df])\n",
    "        return outcomes_fn, highest_probas_fn, to_duplicate_next\n",
    "\n",
    "    def prepare_train_dataset(\n",
    "        self, X_train_arg, y_train_arg, duplicates_per_case, flag\n",
    "    ):\n",
    "        if duplicates_per_case.empty or not flag:\n",
    "            # nothing to dupe or flag is down (skip)\n",
    "            return X_train_arg, y_train_arg\n",
    "\n",
    "        X_train = copy.deepcopy(X_train_arg)\n",
    "        y_train = copy.deepcopy(y_train_arg)\n",
    "\n",
    "        # make 1 full dataset for easy modification\n",
    "        full_test_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "        # for each feature\n",
    "        duplicate_rows = pd.DataFrame(columns=X_train.columns.tolist())\n",
    "        for col_name in duplicates_per_case.axes[0].tolist():\n",
    "            dupes = full_test_data.apply(\n",
    "                lambda row: row[\n",
    "                    full_test_data[col_name].isin([duplicates_per_case[col_name]])\n",
    "                ]\n",
    "            )\n",
    "            duplicate_rows = pd.concat(\n",
    "                [duplicate_rows, dupes], axis=0, ignore_index=True\n",
    "            )\n",
    "        full_test_data = pd.concat(\n",
    "            [full_test_data, duplicate_rows], axis=0, ignore_index=True\n",
    "        )\n",
    "\n",
    "        # print(\"Delta:\", np.shape(full_test_data)[0])\n",
    "\n",
    "        # return X_train, y_train\n",
    "        return (\n",
    "            full_test_data.loc[:, full_test_data.columns != \"labels\"],\n",
    "            full_test_data.loc[:, \"labels\"],\n",
    "        )\n",
    "\n",
    "    def find_next_best_feature(\n",
    "        self,\n",
    "        X_tr,\n",
    "        y_tr,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        unused_feat,\n",
    "        current_feat,\n",
    "        whole_dataset,\n",
    "        ordinal_categoires_order,\n",
    "        cols_ordinal,\n",
    "        cols_one_hot,\n",
    "    ):\n",
    "        kf = KFold(n_splits=self.CV_folds)\n",
    "        accuracy_per_new_feature = pd.DataFrame(\n",
    "            0, index=np.arange(1), columns=unused_feat,\n",
    "        )\n",
    "        for new_feature in unused_feat:\n",
    "            # print(\"Calculating feature: \", new_feature)\n",
    "            sum_of_accuracies = 0\n",
    "            feature_set_to_try = copy.deepcopy(current_feat)\n",
    "            feature_set_to_try.append(new_feature)\n",
    "            dataset_for_encoder = pd.DataFrame(whole_dataset[feature_set_to_try])\n",
    "\n",
    "            for train_index, test_index in kf.split(X_tr):\n",
    "                # create _train, _cv_test, _test splits\n",
    "                # no need to reshuffle it, it's already in random order\n",
    "                # X is a dataframe\n",
    "                X_train, X_cv = (\n",
    "                    X_tr.iloc[train_index],\n",
    "                    X_tr.iloc[test_index],\n",
    "                )\n",
    "\n",
    "                # y is a numpy array\n",
    "                y_train, y_cv = (\n",
    "                    y_tr[train_index],\n",
    "                    y_tr[test_index],\n",
    "                )\n",
    "\n",
    "                # print(\"train: \", train_index, \"test: \", test_index)\n",
    "\n",
    "                # add feture to test to X\n",
    "                X_train_subset = pd.DataFrame(X_train[feature_set_to_try])\n",
    "                X_cv_subset = pd.DataFrame(X_cv[feature_set_to_try])\n",
    "\n",
    "                # make classifier\n",
    "                classifier = self.make_encoding_categorical_bayes(\n",
    "                    ordinal_categoires_order,\n",
    "                    cols_ordinal,\n",
    "                    cols_one_hot,\n",
    "                    dataset_for_encoder,\n",
    "                )\n",
    "\n",
    "                # train classifier\n",
    "                classifier.fit(X_train_subset, y_train)\n",
    "                # predict\n",
    "                y_cv_prediciton = classifier.predict(X_cv_subset)\n",
    "                # judge accuracy of new feature subset\n",
    "                sum_of_accuracies += metrics.accuracy_score(y_cv, y_cv_prediciton)\n",
    "\n",
    "            # save accuracy per new feature\n",
    "            accuracy_per_new_feature[new_feature] = sum_of_accuracies / self.CV_folds\n",
    "            # print(\"Finished calculations for feature: \",new_feature,\"Accuracy: \",accuracy_per_new_feature[new_feature][0])\n",
    "\n",
    "        return accuracy_per_new_feature\n",
    "\n",
    "    def make_encoding_categorical_bayes(\n",
    "        self, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset\n",
    "    ):\n",
    "        return EncodingCategoricalBayes(\n",
    "            # classifier=CategoricalNB(),\n",
    "            ordinal_categories_order=order_of_ordinal_categories,\n",
    "            ordinal_columns=cols_ordinal,\n",
    "            one_hot_columns=cols_one_hot,\n",
    "            dataset=whole_dataset,\n",
    "        )\n",
    "\n",
    "    def get_classification_costs(self, list_of_categories):\n",
    "        return self.classification_costs[\n",
    "            self.classification_costs.columns.intersection(list_of_categories)\n",
    "        ].sum(axis=1)[0]\n",
    "\n",
    "\n",
    "print(\"Sequential Forward Feature Selector is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "36541769",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of loop number:  0\n",
      "Picked feature:  gill_color\n",
      "Current feature set:  ['gill_color']\n",
      "End of loop  0\n",
      "Classified classes:  673 / 1625 41.42 %\n",
      "Start of loop number:  1\n",
      "Picked feature:  stalk_surface_above_ring\n",
      "Current feature set:  ['gill_color', 'stalk_surface_above_ring']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4796/44410623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m results = selector.sequential_predict(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mencoded_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4796/1874336391.py\u001b[0m in \u001b[0;36msequential_predict\u001b[1;34m(self, X_train_original, y_train_original, X_test_original, y_test_original, ordinal_categoires_order, cols_ordinal, cols_one_hot, whole_dataset, data_duplication_flag)\u001b[0m\n\u001b[0;32m     80\u001b[0m             )\n\u001b[0;32m     81\u001b[0m             \u001b[1;31m# define duplicates before end of refactorization and moving forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             outcomes, highest_probas, duplicates = self.predict_proba_wrapper(\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mX_train_subset_full\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4796/1874336391.py\u001b[0m in \u001b[0;36mpredict_proba_wrapper\u001b[1;34m(self, classifier, X_train, y_train, X_test, duplicates, flag, unknown_feature)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             )\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_mod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mdf_row_entry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_entry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4796/546634926.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategoricalNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_categories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_column_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# return self.classifier.fit(X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;31m# need the transformed data) to have consistent output type in predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[1;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[0;32m    432\u001b[0m             self._iter(fitted=fitted, replace_strings=True))\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             return Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    435\u001b[0m                 delayed(func)(\n\u001b[0;32m    436\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    831\u001b[0m                 \u001b[0mbig_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                 \u001b[0mislice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbig_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    435\u001b[0m                 delayed(func)(\n\u001b[0;32m    436\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m                     \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# check whether we should index with loc or iloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    804\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m             \u001b[1;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[1;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1151\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m         return self.obj._reindex_with_indexers(\n\u001b[0m\u001b[0;32m   1095\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4882\u001b[0m             \u001b[1;31m# TODO: speed up on homogeneous DataFrame objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4883\u001b[1;33m             new_data = new_data.reindex_indexer(\n\u001b[0m\u001b[0;32m   4884\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4885\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[0;32m    677\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_slice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0monly_slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice)\u001b[0m\n\u001b[0;32m    817\u001b[0m                             \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                         \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m                         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1142\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m   1145\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 140;\n",
       "                var nbb_unformatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nresults = selector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,  # order of ordinal categories\\n    cols_ordinal,  # list of ordinal columns in whole data\\n    cols_one_hot,  # list of one hot columns in whole data\\n    X_cat,  # encoder dataset\\n    True,  # feature duplication when classifying\\n)\\n\\nprint(\\\"Done\\\")\\n# print(results)\";\n",
       "                var nbb_formatted_code = \"# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\\n\\nselector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\\n\\n# Train the model using the training sets\\nresults = selector.sequential_predict(\\n    X_train,\\n    encoded_y_train,\\n    X_test,\\n    encoded_y_test,\\n    order_of_ordinal_categories,  # order of ordinal categories\\n    cols_ordinal,  # list of ordinal columns in whole data\\n    cols_one_hot,  # list of one hot columns in whole data\\n    X_cat,  # encoder dataset\\n    True,  # feature duplication when classifying\\n)\\n\\nprint(\\\"Done\\\")\\n# print(results)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Bayes Classifier || requires min_categories due to a bug with indexes, reporting the bug added to TODO\n",
    "\n",
    "selector = SequentialForwardFeatureSelector(dataset_costs, 10, 0.05)\n",
    "\n",
    "# Train the model using the training sets\n",
    "results = selector.sequential_predict(\n",
    "    X_train,\n",
    "    encoded_y_train,\n",
    "    X_test,\n",
    "    encoded_y_test,\n",
    "    order_of_ordinal_categories,  # order of ordinal categories\n",
    "    cols_ordinal,  # list of ordinal columns in whole data\n",
    "    cols_one_hot,  # list of one hot columns in whole data\n",
    "    X_cat,  # encoder dataset\n",
    "    True,  # feature duplication when classifying\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d835b50d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.8 %\n",
      "F1 score: 88.65863157084584 %\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 141;\n",
       "                var nbb_unformatted_code = \"# warning: not sorted!\\ny_pred = results[\\\"outcome\\\"].sort_index()\\ny_test.sort_index(inplace=True)\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_formatted_code = \"# warning: not sorted!\\ny_pred = results[\\\"outcome\\\"].sort_index()\\ny_test.sort_index(inplace=True)\\nprint(\\\"Accuracy:\\\", metrics.accuracy_score(y_test, y_pred) * 100, \\\"%\\\")\\nprint(\\\"F1 score:\\\", metrics.f1_score(y_test, y_pred, average=\\\"weighted\\\") * 100, \\\"%\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# warning: not sorted!\n",
    "y_pred = results[\"outcome\"].sort_index()\n",
    "y_test.sort_index(inplace=True)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred) * 100, \"%\")\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred, average=\"weighted\") * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduct some useful metrics:\n",
    "# mean cost\n",
    "# median cost\n",
    "# difference in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "file_name = 'results independent feature selection'\n",
    "result.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
